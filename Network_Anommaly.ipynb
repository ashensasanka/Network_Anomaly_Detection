{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashen\\AppData\\Local\\Temp\\ipykernel_348\\272811689.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 35em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "\n",
    "# Configure Jupyter Notebook\n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.max_rows', 500) \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "# pd.set_option('max_colwidth', -1)\n",
    "display(HTML(\"<style>div.output_scroll { height: 35em; }</style>\"))\n",
    "\n",
    "reload(plt)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# configure plotly graph objects\n",
    "pio.renderers.default = 'iframe'\n",
    "# pio.renderers.default = 'vscode'\n",
    "\n",
    "pio.templates[\"ck_template\"] = go.layout.Template(\n",
    "    layout_colorway = px.colors.sequential.Viridis, \n",
    "#     layout_hovermode = 'closest',\n",
    "#     layout_hoverdistance = -1,\n",
    "    layout_autosize=False,\n",
    "    layout_width=800,\n",
    "    layout_height=600,\n",
    "    layout_font = dict(family=\"Calibri Light\"),\n",
    "    layout_title_font = dict(family=\"Calibri\"),\n",
    "    layout_hoverlabel_font = dict(family=\"Calibri Light\"),\n",
    "#     plot_bgcolor=\"white\",\n",
    ")\n",
    " \n",
    "# pio.templates.default = 'seaborn+ck_template+gridon'\n",
    "pio.templates.default = 'ck_template+gridon'\n",
    "# pio.templates.default = 'seaborn+gridon'\n",
    "# pio.templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('UNSW_NB15_training-set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175341 entries, 0 to 175340\n",
      "Data columns (total 45 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   id                 175341 non-null  int64  \n",
      " 1   dur                175341 non-null  float64\n",
      " 2   proto              175341 non-null  object \n",
      " 3   service            175341 non-null  object \n",
      " 4   state              175341 non-null  object \n",
      " 5   spkts              175341 non-null  int64  \n",
      " 6   dpkts              175341 non-null  int64  \n",
      " 7   sbytes             175341 non-null  int64  \n",
      " 8   dbytes             175341 non-null  int64  \n",
      " 9   rate               175341 non-null  float64\n",
      " 10  sttl               175341 non-null  int64  \n",
      " 11  dttl               175341 non-null  int64  \n",
      " 12  sload              175341 non-null  float64\n",
      " 13  dload              175341 non-null  float64\n",
      " 14  sloss              175341 non-null  int64  \n",
      " 15  dloss              175341 non-null  int64  \n",
      " 16  sinpkt             175341 non-null  float64\n",
      " 17  dinpkt             175341 non-null  float64\n",
      " 18  sjit               175341 non-null  float64\n",
      " 19  djit               175341 non-null  float64\n",
      " 20  swin               175341 non-null  int64  \n",
      " 21  stcpb              175341 non-null  int64  \n",
      " 22  dtcpb              175341 non-null  int64  \n",
      " 23  dwin               175341 non-null  int64  \n",
      " 24  tcprtt             175341 non-null  float64\n",
      " 25  synack             175341 non-null  float64\n",
      " 26  ackdat             175341 non-null  float64\n",
      " 27  smean              175341 non-null  int64  \n",
      " 28  dmean              175341 non-null  int64  \n",
      " 29  trans_depth        175341 non-null  int64  \n",
      " 30  response_body_len  175341 non-null  int64  \n",
      " 31  ct_srv_src         175341 non-null  int64  \n",
      " 32  ct_state_ttl       175341 non-null  int64  \n",
      " 33  ct_dst_ltm         175341 non-null  int64  \n",
      " 34  ct_src_dport_ltm   175341 non-null  int64  \n",
      " 35  ct_dst_sport_ltm   175341 non-null  int64  \n",
      " 36  ct_dst_src_ltm     175341 non-null  int64  \n",
      " 37  is_ftp_login       175341 non-null  int64  \n",
      " 38  ct_ftp_cmd         175341 non-null  int64  \n",
      " 39  ct_flw_http_mthd   175341 non-null  int64  \n",
      " 40  ct_src_ltm         175341 non-null  int64  \n",
      " 41  ct_srv_dst         175341 non-null  int64  \n",
      " 42  is_sm_ips_ports    175341 non-null  int64  \n",
      " 43  attack_cat         175341 non-null  object \n",
      " 44  label              175341 non-null  int64  \n",
      "dtypes: float64(11), int64(30), object(4)\n",
      "memory usage: 60.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sload</th>\n",
       "      <th>dload</th>\n",
       "      <th>sloss</th>\n",
       "      <th>dloss</th>\n",
       "      <th>sinpkt</th>\n",
       "      <th>dinpkt</th>\n",
       "      <th>sjit</th>\n",
       "      <th>djit</th>\n",
       "      <th>swin</th>\n",
       "      <th>stcpb</th>\n",
       "      <th>dtcpb</th>\n",
       "      <th>dwin</th>\n",
       "      <th>tcprtt</th>\n",
       "      <th>synack</th>\n",
       "      <th>ackdat</th>\n",
       "      <th>smean</th>\n",
       "      <th>dmean</th>\n",
       "      <th>trans_depth</th>\n",
       "      <th>response_body_len</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_state_ttl</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.121478</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>258</td>\n",
       "      <td>172</td>\n",
       "      <td>74.087490</td>\n",
       "      <td>252</td>\n",
       "      <td>254</td>\n",
       "      <td>14158.942380</td>\n",
       "      <td>8495.365234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.295600</td>\n",
       "      <td>8.375000</td>\n",
       "      <td>30.177547</td>\n",
       "      <td>11.830604</td>\n",
       "      <td>255</td>\n",
       "      <td>621772692</td>\n",
       "      <td>2202533631</td>\n",
       "      <td>255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.649902</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>734</td>\n",
       "      <td>42014</td>\n",
       "      <td>78.473372</td>\n",
       "      <td>62</td>\n",
       "      <td>252</td>\n",
       "      <td>8395.112305</td>\n",
       "      <td>503571.312500</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>49.915000</td>\n",
       "      <td>15.432865</td>\n",
       "      <td>61.426934</td>\n",
       "      <td>1387.778330</td>\n",
       "      <td>255</td>\n",
       "      <td>1417884146</td>\n",
       "      <td>3077387971</td>\n",
       "      <td>255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52</td>\n",
       "      <td>1106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.623129</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>364</td>\n",
       "      <td>13186</td>\n",
       "      <td>14.170161</td>\n",
       "      <td>62</td>\n",
       "      <td>252</td>\n",
       "      <td>1572.271851</td>\n",
       "      <td>60929.230470</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>231.875571</td>\n",
       "      <td>102.737203</td>\n",
       "      <td>17179.586860</td>\n",
       "      <td>11420.926230</td>\n",
       "      <td>255</td>\n",
       "      <td>2116150707</td>\n",
       "      <td>2963114973</td>\n",
       "      <td>255</td>\n",
       "      <td>0.111897</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.050439</td>\n",
       "      <td>46</td>\n",
       "      <td>824</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.681642</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>628</td>\n",
       "      <td>770</td>\n",
       "      <td>13.677108</td>\n",
       "      <td>62</td>\n",
       "      <td>252</td>\n",
       "      <td>2740.178955</td>\n",
       "      <td>3358.622070</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>152.876547</td>\n",
       "      <td>90.235726</td>\n",
       "      <td>259.080172</td>\n",
       "      <td>4991.784669</td>\n",
       "      <td>255</td>\n",
       "      <td>1107119177</td>\n",
       "      <td>1047442890</td>\n",
       "      <td>255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.449454</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>534</td>\n",
       "      <td>268</td>\n",
       "      <td>33.373826</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>8561.499023</td>\n",
       "      <td>3987.059814</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>47.750333</td>\n",
       "      <td>75.659602</td>\n",
       "      <td>2415.837634</td>\n",
       "      <td>115.807000</td>\n",
       "      <td>255</td>\n",
       "      <td>2436137549</td>\n",
       "      <td>1977154190</td>\n",
       "      <td>255</td>\n",
       "      <td>0.128381</td>\n",
       "      <td>0.071147</td>\n",
       "      <td>0.057234</td>\n",
       "      <td>53</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.380537</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>534</td>\n",
       "      <td>268</td>\n",
       "      <td>39.417980</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>10112.025390</td>\n",
       "      <td>4709.134766</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>39.928778</td>\n",
       "      <td>52.241000</td>\n",
       "      <td>2223.730342</td>\n",
       "      <td>82.550500</td>\n",
       "      <td>255</td>\n",
       "      <td>3984155503</td>\n",
       "      <td>1796040391</td>\n",
       "      <td>255</td>\n",
       "      <td>0.172934</td>\n",
       "      <td>0.119331</td>\n",
       "      <td>0.053603</td>\n",
       "      <td>53</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.637109</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>534</td>\n",
       "      <td>354</td>\n",
       "      <td>26.683033</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>6039.783203</td>\n",
       "      <td>3892.583740</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>68.267778</td>\n",
       "      <td>81.137711</td>\n",
       "      <td>4286.828570</td>\n",
       "      <td>119.422719</td>\n",
       "      <td>255</td>\n",
       "      <td>1787309226</td>\n",
       "      <td>1767180493</td>\n",
       "      <td>255</td>\n",
       "      <td>0.143337</td>\n",
       "      <td>0.069136</td>\n",
       "      <td>0.074201</td>\n",
       "      <td>53</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.521584</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>534</td>\n",
       "      <td>354</td>\n",
       "      <td>32.593026</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>7377.527344</td>\n",
       "      <td>4754.747070</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>55.794000</td>\n",
       "      <td>66.054141</td>\n",
       "      <td>3770.580726</td>\n",
       "      <td>118.962633</td>\n",
       "      <td>255</td>\n",
       "      <td>205985702</td>\n",
       "      <td>316006300</td>\n",
       "      <td>255</td>\n",
       "      <td>0.116615</td>\n",
       "      <td>0.059195</td>\n",
       "      <td>0.057420</td>\n",
       "      <td>53</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.542905</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>534</td>\n",
       "      <td>354</td>\n",
       "      <td>31.313031</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>7087.796387</td>\n",
       "      <td>4568.018555</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>60.210889</td>\n",
       "      <td>68.109000</td>\n",
       "      <td>4060.625597</td>\n",
       "      <td>106.611547</td>\n",
       "      <td>255</td>\n",
       "      <td>884094874</td>\n",
       "      <td>3410317203</td>\n",
       "      <td>255</td>\n",
       "      <td>0.118584</td>\n",
       "      <td>0.066133</td>\n",
       "      <td>0.052451</td>\n",
       "      <td>53</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.258687</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>534</td>\n",
       "      <td>268</td>\n",
       "      <td>57.985135</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>14875.120120</td>\n",
       "      <td>6927.291016</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.505111</td>\n",
       "      <td>39.106801</td>\n",
       "      <td>1413.686415</td>\n",
       "      <td>57.200395</td>\n",
       "      <td>255</td>\n",
       "      <td>3368447996</td>\n",
       "      <td>584859215</td>\n",
       "      <td>255</td>\n",
       "      <td>0.087934</td>\n",
       "      <td>0.063116</td>\n",
       "      <td>0.024818</td>\n",
       "      <td>53</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       dur proto service state  spkts  dpkts  sbytes  dbytes       rate  sttl  dttl         sload          dload  sloss  dloss      sinpkt      dinpkt          sjit          djit  swin       stcpb       dtcpb  dwin    tcprtt    synack    ackdat  smean  dmean  trans_depth  response_body_len  ct_srv_src  ct_state_ttl  ct_dst_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports attack_cat  label\n",
       "0   1  0.121478   tcp       -   FIN      6      4     258     172  74.087490   252   254  14158.942380    8495.365234      0      0   24.295600    8.375000     30.177547     11.830604   255   621772692  2202533631   255  0.000000  0.000000  0.000000     43     43            0                  0           1             0           1                 1                 1               1             0           0                 0           1           1                0     Normal      0\n",
       "1   2  0.649902   tcp       -   FIN     14     38     734   42014  78.473372    62   252   8395.112305  503571.312500      2     17   49.915000   15.432865     61.426934   1387.778330   255  1417884146  3077387971   255  0.000000  0.000000  0.000000     52   1106            0                  0          43             1           1                 1                 1               2             0           0                 0           1           6                0     Normal      0\n",
       "2   3  1.623129   tcp       -   FIN      8     16     364   13186  14.170161    62   252   1572.271851   60929.230470      1      6  231.875571  102.737203  17179.586860  11420.926230   255  2116150707  2963114973   255  0.111897  0.061458  0.050439     46    824            0                  0           7             1           2                 1                 1               3             0           0                 0           2           6                0     Normal      0\n",
       "3   4  1.681642   tcp     ftp   FIN     12     12     628     770  13.677108    62   252   2740.178955    3358.622070      1      3  152.876547   90.235726    259.080172   4991.784669   255  1107119177  1047442890   255  0.000000  0.000000  0.000000     52     64            0                  0           1             1           2                 1                 1               3             1           1                 0           2           1                0     Normal      0\n",
       "4   5  0.449454   tcp       -   FIN     10      6     534     268  33.373826   254   252   8561.499023    3987.059814      2      1   47.750333   75.659602   2415.837634    115.807000   255  2436137549  1977154190   255  0.128381  0.071147  0.057234     53     45            0                  0          43             1           2                 2                 1              40             0           0                 0           2          39                0     Normal      0\n",
       "5   6  0.380537   tcp       -   FIN     10      6     534     268  39.417980   254   252  10112.025390    4709.134766      2      1   39.928778   52.241000   2223.730342     82.550500   255  3984155503  1796040391   255  0.172934  0.119331  0.053603     53     45            0                  0          43             1           2                 2                 1              40             0           0                 0           2          39                0     Normal      0\n",
       "6   7  0.637109   tcp       -   FIN     10      8     534     354  26.683033   254   252   6039.783203    3892.583740      2      1   68.267778   81.137711   4286.828570    119.422719   255  1787309226  1767180493   255  0.143337  0.069136  0.074201     53     44            0                  0          43             1           1                 1                 1              40             0           0                 0           1          39                0     Normal      0\n",
       "7   8  0.521584   tcp       -   FIN     10      8     534     354  32.593026   254   252   7377.527344    4754.747070      2      1   55.794000   66.054141   3770.580726    118.962633   255   205985702   316006300   255  0.116615  0.059195  0.057420     53     44            0                  0          43             1           3                 3                 1              40             0           0                 0           3          39                0     Normal      0\n",
       "8   9  0.542905   tcp       -   FIN     10      8     534     354  31.313031   254   252   7087.796387    4568.018555      2      1   60.210889   68.109000   4060.625597    106.611547   255   884094874  3410317203   255  0.118584  0.066133  0.052451     53     44            0                  0          43             1           3                 3                 1              40             0           0                 0           3          39                0     Normal      0\n",
       "9  10  0.258687   tcp       -   FIN     10      6     534     268  57.985135   254   252  14875.120120    6927.291016      2      1   27.505111   39.106801   1413.686415     57.200395   255  3368447996   584859215   255  0.087934  0.063116  0.024818     53     45            0                  0          43             1           3                 3                 1              40             0           0                 0           3          39                0     Normal      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sload</th>\n",
       "      <th>dload</th>\n",
       "      <th>sloss</th>\n",
       "      <th>dloss</th>\n",
       "      <th>sinpkt</th>\n",
       "      <th>dinpkt</th>\n",
       "      <th>sjit</th>\n",
       "      <th>djit</th>\n",
       "      <th>swin</th>\n",
       "      <th>stcpb</th>\n",
       "      <th>dtcpb</th>\n",
       "      <th>dwin</th>\n",
       "      <th>tcprtt</th>\n",
       "      <th>synack</th>\n",
       "      <th>ackdat</th>\n",
       "      <th>smean</th>\n",
       "      <th>dmean</th>\n",
       "      <th>trans_depth</th>\n",
       "      <th>response_body_len</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_state_ttl</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341</td>\n",
       "      <td>175341</td>\n",
       "      <td>175341</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>1.753410e+05</td>\n",
       "      <td>1.753410e+05</td>\n",
       "      <td>1.753410e+05</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>1.753410e+05</td>\n",
       "      <td>1.753410e+05</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>1.753410e+05</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>1.753410e+05</td>\n",
       "      <td>1.753410e+05</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>1.753410e+05</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "      <td>175341.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>133</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>79946</td>\n",
       "      <td>94168</td>\n",
       "      <td>82275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.359389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.298664</td>\n",
       "      <td>18.969591</td>\n",
       "      <td>8.844844e+03</td>\n",
       "      <td>1.492892e+04</td>\n",
       "      <td>9.540619e+04</td>\n",
       "      <td>179.546997</td>\n",
       "      <td>79.609567</td>\n",
       "      <td>7.345403e+07</td>\n",
       "      <td>6.712056e+05</td>\n",
       "      <td>4.953000</td>\n",
       "      <td>6.948010</td>\n",
       "      <td>985.976864</td>\n",
       "      <td>88.216296</td>\n",
       "      <td>4.976254e+03</td>\n",
       "      <td>604.353826</td>\n",
       "      <td>116.257339</td>\n",
       "      <td>9.692504e+08</td>\n",
       "      <td>9.688770e+08</td>\n",
       "      <td>115.013625</td>\n",
       "      <td>0.041396</td>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.020375</td>\n",
       "      <td>136.751769</td>\n",
       "      <td>124.173382</td>\n",
       "      <td>0.105982</td>\n",
       "      <td>2.144292e+03</td>\n",
       "      <td>9.306437</td>\n",
       "      <td>1.304179</td>\n",
       "      <td>6.193936</td>\n",
       "      <td>5.383538</td>\n",
       "      <td>4.206255</td>\n",
       "      <td>8.729881</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.133066</td>\n",
       "      <td>6.955789</td>\n",
       "      <td>9.100758</td>\n",
       "      <td>0.015752</td>\n",
       "      <td>0.680622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.480249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136.887597</td>\n",
       "      <td>110.258271</td>\n",
       "      <td>1.747656e+05</td>\n",
       "      <td>1.436542e+05</td>\n",
       "      <td>1.654010e+05</td>\n",
       "      <td>102.940011</td>\n",
       "      <td>110.506863</td>\n",
       "      <td>1.883574e+08</td>\n",
       "      <td>2.421312e+06</td>\n",
       "      <td>66.005059</td>\n",
       "      <td>52.732999</td>\n",
       "      <td>7242.245841</td>\n",
       "      <td>987.093195</td>\n",
       "      <td>4.496585e+04</td>\n",
       "      <td>4061.043281</td>\n",
       "      <td>127.001024</td>\n",
       "      <td>1.355264e+09</td>\n",
       "      <td>1.354000e+09</td>\n",
       "      <td>126.886530</td>\n",
       "      <td>0.079354</td>\n",
       "      <td>0.043400</td>\n",
       "      <td>0.040506</td>\n",
       "      <td>204.677360</td>\n",
       "      <td>258.317056</td>\n",
       "      <td>0.776911</td>\n",
       "      <td>5.420797e+04</td>\n",
       "      <td>10.704331</td>\n",
       "      <td>0.954406</td>\n",
       "      <td>8.052476</td>\n",
       "      <td>8.047104</td>\n",
       "      <td>5.783585</td>\n",
       "      <td>10.956186</td>\n",
       "      <td>0.126048</td>\n",
       "      <td>0.126048</td>\n",
       "      <td>0.701208</td>\n",
       "      <td>8.321493</td>\n",
       "      <td>10.756952</td>\n",
       "      <td>0.124516</td>\n",
       "      <td>0.466237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.140000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.278614e+01</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.305334e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.300000e+02</td>\n",
       "      <td>1.640000e+02</td>\n",
       "      <td>3.225807e+03</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>8.796748e+05</td>\n",
       "      <td>1.447023e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279733</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.668069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.418000e+03</td>\n",
       "      <td>1.102000e+03</td>\n",
       "      <td>1.250000e+05</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>8.888889e+07</td>\n",
       "      <td>2.784487e+04</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.156896</td>\n",
       "      <td>51.053000</td>\n",
       "      <td>2.513295e+03</td>\n",
       "      <td>114.990625</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.916651e+09</td>\n",
       "      <td>1.913675e+09</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.065481</td>\n",
       "      <td>0.023268</td>\n",
       "      <td>0.038906</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>59.999989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9616.000000</td>\n",
       "      <td>10974.000000</td>\n",
       "      <td>1.296523e+07</td>\n",
       "      <td>1.465555e+07</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>5.988000e+09</td>\n",
       "      <td>2.242273e+07</td>\n",
       "      <td>4803.000000</td>\n",
       "      <td>5484.000000</td>\n",
       "      <td>84371.496000</td>\n",
       "      <td>56716.824000</td>\n",
       "      <td>1.460480e+06</td>\n",
       "      <td>289388.269700</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>4.294959e+09</td>\n",
       "      <td>4.294882e+09</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>2.518893</td>\n",
       "      <td>2.100352</td>\n",
       "      <td>1.520884</td>\n",
       "      <td>1504.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>6.558056e+06</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  dur   proto service   state          spkts          dpkts        sbytes        dbytes          rate           sttl           dttl         sload         dload          sloss          dloss         sinpkt         dinpkt          sjit           djit           swin         stcpb         dtcpb           dwin         tcprtt         synack         ackdat          smean          dmean    trans_depth  response_body_len     ct_srv_src   ct_state_ttl     ct_dst_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm   is_ftp_login     ct_ftp_cmd  ct_flw_http_mthd     ct_src_ltm     ct_srv_dst  is_sm_ips_ports          label\n",
       "count   175341.000000  175341  175341  175341  175341.000000  175341.000000  1.753410e+05  1.753410e+05  1.753410e+05  175341.000000  175341.000000  1.753410e+05  1.753410e+05  175341.000000  175341.000000  175341.000000  175341.000000  1.753410e+05  175341.000000  175341.000000  1.753410e+05  1.753410e+05  175341.000000  175341.000000  175341.000000  175341.000000  175341.000000  175341.000000  175341.000000       1.753410e+05  175341.000000  175341.000000  175341.000000     175341.000000     175341.000000   175341.000000  175341.000000  175341.000000     175341.000000  175341.000000  175341.000000    175341.000000  175341.000000\n",
       "unique            NaN     133      13       9            NaN            NaN           NaN           NaN           NaN            NaN            NaN           NaN           NaN            NaN            NaN            NaN            NaN           NaN            NaN            NaN           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                NaN            NaN            NaN            NaN               NaN               NaN             NaN            NaN            NaN               NaN            NaN            NaN              NaN            NaN\n",
       "top               NaN     tcp       -     INT            NaN            NaN           NaN           NaN           NaN            NaN            NaN           NaN           NaN            NaN            NaN            NaN            NaN           NaN            NaN            NaN           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                NaN            NaN            NaN            NaN               NaN               NaN             NaN            NaN            NaN               NaN            NaN            NaN              NaN            NaN\n",
       "freq              NaN   79946   94168   82275            NaN            NaN           NaN           NaN           NaN            NaN            NaN           NaN           NaN            NaN            NaN            NaN            NaN           NaN            NaN            NaN           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                NaN            NaN            NaN            NaN               NaN               NaN             NaN            NaN            NaN               NaN            NaN            NaN              NaN            NaN\n",
       "mean         1.359389     NaN     NaN     NaN      20.298664      18.969591  8.844844e+03  1.492892e+04  9.540619e+04     179.546997      79.609567  7.345403e+07  6.712056e+05       4.953000       6.948010     985.976864      88.216296  4.976254e+03     604.353826     116.257339  9.692504e+08  9.688770e+08     115.013625       0.041396       0.021020       0.020375     136.751769     124.173382       0.105982       2.144292e+03       9.306437       1.304179       6.193936          5.383538          4.206255        8.729881       0.014948       0.014948          0.133066       6.955789       9.100758         0.015752       0.680622\n",
       "std          6.480249     NaN     NaN     NaN     136.887597     110.258271  1.747656e+05  1.436542e+05  1.654010e+05     102.940011     110.506863  1.883574e+08  2.421312e+06      66.005059      52.732999    7242.245841     987.093195  4.496585e+04    4061.043281     127.001024  1.355264e+09  1.354000e+09     126.886530       0.079354       0.043400       0.040506     204.677360     258.317056       0.776911       5.420797e+04      10.704331       0.954406       8.052476          8.047104          5.783585       10.956186       0.126048       0.126048          0.701208       8.321493      10.756952         0.124516       0.466237\n",
       "min          0.000000     NaN     NaN     NaN       1.000000       0.000000  2.800000e+01  0.000000e+00  0.000000e+00       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000  0.000000e+00       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000      28.000000       0.000000       0.000000       0.000000e+00       1.000000       0.000000       1.000000          1.000000          1.000000        1.000000       0.000000       0.000000          0.000000       1.000000       1.000000         0.000000       0.000000\n",
       "25%          0.000008     NaN     NaN     NaN       2.000000       0.000000  1.140000e+02  0.000000e+00  3.278614e+01      62.000000       0.000000  1.305334e+04  0.000000e+00       0.000000       0.000000       0.008000       0.000000  0.000000e+00       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000      57.000000       0.000000       0.000000       0.000000e+00       2.000000       1.000000       1.000000          1.000000          1.000000        1.000000       0.000000       0.000000          0.000000       2.000000       2.000000         0.000000       0.000000\n",
       "50%          0.001582     NaN     NaN     NaN       2.000000       2.000000  4.300000e+02  1.640000e+02  3.225807e+03     254.000000      29.000000  8.796748e+05  1.447023e+03       0.000000       0.000000       0.279733       0.006000  0.000000e+00       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000      73.000000      44.000000       0.000000       0.000000e+00       5.000000       1.000000       2.000000          1.000000          1.000000        3.000000       0.000000       0.000000          0.000000       3.000000       4.000000         0.000000       1.000000\n",
       "75%          0.668069     NaN     NaN     NaN      12.000000      10.000000  1.418000e+03  1.102000e+03  1.250000e+05     254.000000     252.000000  8.888889e+07  2.784487e+04       3.000000       2.000000      55.156896      51.053000  2.513295e+03     114.990625     255.000000  1.916651e+09  1.913675e+09     255.000000       0.065481       0.023268       0.038906     100.000000      89.000000       0.000000       0.000000e+00      12.000000       2.000000       7.000000          5.000000          3.000000       12.000000       0.000000       0.000000          0.000000       9.000000      12.000000         0.000000       1.000000\n",
       "max         59.999989     NaN     NaN     NaN    9616.000000   10974.000000  1.296523e+07  1.465555e+07  1.000000e+06     255.000000     254.000000  5.988000e+09  2.242273e+07    4803.000000    5484.000000   84371.496000   56716.824000  1.460480e+06  289388.269700     255.000000  4.294959e+09  4.294882e+09     255.000000       2.518893       2.100352       1.520884    1504.000000    1458.000000     172.000000       6.558056e+06      63.000000       6.000000      51.000000         51.000000         46.000000       65.000000       4.000000       4.000000         30.000000      60.000000      62.000000         1.000000       1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_drop = ['id','attack_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(list_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clamp extreme Values\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "df_numeric.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG =0\n",
    "\n",
    "for feature in df_numeric.columns:\n",
    "    if DEBUG == 1:\n",
    "        print(feature)\n",
    "        print('max = '+str(df_numeric[feature].max()))\n",
    "        print('75th = '+str(df_numeric[feature].quantile(0.95)))\n",
    "        print('median = '+str(df_numeric[feature].median()))\n",
    "        print(df_numeric[feature].max()>10*df_numeric[feature].median())\n",
    "        print('----------------------------------------------------')\n",
    "    if df_numeric[feature].max()>10*df_numeric[feature].median() and df_numeric[feature].max()>10 :\n",
    "        df[feature] = np.where(df[feature]<df[feature].quantile(0.95), df[feature], df[feature].quantile(0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "df_numeric.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "df_before = df_numeric.copy()\n",
    "DEBUG = 0\n",
    "for feature in df_numeric.columns:\n",
    "    if DEBUG == 1:\n",
    "        print(feature)\n",
    "        print('nunique = '+str(df_numeric[feature].nunique()))\n",
    "        print(df_numeric[feature].nunique()>50)\n",
    "        print('----------------------------------------------------')\n",
    "    if df_numeric[feature].nunique()>50:\n",
    "        if df_numeric[feature].min()==0:\n",
    "            df[feature] = np.log(df[feature]+1)\n",
    "        else:\n",
    "            df[feature] = np.log(df[feature])\n",
    "\n",
    "df_numeric = df.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.select_dtypes(exclude=[np.number])\n",
    "df_cat.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = 0\n",
    "for feature in df_cat.columns:\n",
    "    if DEBUG == 1:\n",
    "        print(feature)\n",
    "        print('nunique = '+str(df_cat[feature].nunique()))\n",
    "        print(df_cat[feature].nunique()>6)\n",
    "        print(sum(df[feature].isin(df[feature].value_counts().head().index)))\n",
    "        print('----------------------------------------------------')\n",
    "    \n",
    "    if df_cat[feature].nunique()>6:\n",
    "        df[feature] = np.where(df[feature].isin(df[feature].value_counts().head().index), df[feature], '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.select_dtypes(exclude=[np.number])\n",
    "df_cat.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['proto'].value_counts().head().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['proto'].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "best_features = SelectKBest(score_func=chi2,k='all')\n",
    "\n",
    "X = df.iloc[:,4:-2]\n",
    "y = df.iloc[:,-1]\n",
    "fit = best_features.fit(X,y)\n",
    "\n",
    "df_scores=pd.DataFrame(fit.scores_)\n",
    "df_col=pd.DataFrame(X.columns)\n",
    "\n",
    "feature_score=pd.concat([df_col,df_scores],axis=1)\n",
    "feature_score.columns=['feature','score']\n",
    "feature_score.sort_values(by=['score'],ascending=True,inplace=True)\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "            x=feature_score['score'][0:21],\n",
    "            y=feature_score['feature'][0:21],\n",
    "            orientation='h'))\n",
    "\n",
    "fig.update_layout(title=\"Top 20 Features\",\n",
    "                  height=1200,\n",
    "                  showlegend=False,\n",
    "                 )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()\n",
    "feature_names = list(X.columns)\n",
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1,2,3])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in list(df_cat['state'].value_counts().index)[::-1][1:]:\n",
    "    feature_names.insert(0,label)\n",
    "    \n",
    "for label in list(df_cat['service'].value_counts().index)[::-1][1:]:\n",
    "    feature_names.insert(0,label)\n",
    "    \n",
    "for label in list(df_cat['proto'].value_counts().index)[::-1][1:]:\n",
    "    feature_names.insert(0,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:, 18:] = sc.fit_transform(X_train[:, 18:])\n",
    "X_test[:, 18:] = sc.transform(X_test[:, 18:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay # will plot the confusion matrix\n",
    "import time\n",
    "model_performance = pd.DataFrame(columns=['Accuracy','Recall','Precision','F1-Score','time to train','time to predict','total time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistical Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "start = time.time()\n",
    "model = LogisticRegression().fit(X_train,y_train)\n",
    "end_train = time.time()\n",
    "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
    "end_predict = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_predictions)\n",
    "recall = recall_score(y_test, y_predictions, average='weighted')\n",
    "precision = precision_score(y_test, y_predictions, average='weighted')\n",
    "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
    "\n",
    "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
    "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
    "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
    "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
    "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
    "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
    "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
    "model_performance.loc['Logistic'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=5,5 \n",
    "sns.set_style(\"white\")\n",
    "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "start = time.time()\n",
    "model = KNeighborsClassifier(n_neighbors=3).fit(X_train,y_train)\n",
    "end_train = time.time()\n",
    "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
    "end_predict = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_predictions)\n",
    "recall = recall_score(y_test, y_predictions, average='weighted')\n",
    "precision = precision_score(y_test, y_predictions, average='weighted')\n",
    "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
    "\n",
    "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
    "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
    "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
    "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
    "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
    "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
    "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
    "model_performance.loc['kNN'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=5,5 \n",
    "sns.set_style(\"white\")\n",
    "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "start = time.time()\n",
    "model = DecisionTreeClassifier().fit(X_train,y_train)\n",
    "end_train = time.time()\n",
    "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
    "end_predict = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_predictions)\n",
    "recall = recall_score(y_test, y_predictions, average='weighted')\n",
    "precision = precision_score(y_test, y_predictions, average='weighted')\n",
    "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
    "\n",
    "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
    "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
    "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
    "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
    "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
    "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
    "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
    "model_performance.loc['Decision Tree'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=5,5 \n",
    "sns.set_style(\"white\")\n",
    "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=10,10\n",
    "sns.set_style(\"white\")\n",
    "feat_importances = pd.Series(model.feature_importances_, index=feature_names)\n",
    "feat_importances = feat_importances.groupby(level=0).mean()\n",
    "feat_importances.nlargest(20).plot(kind='barh').invert_yaxis()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "start = time.time()\n",
    "model = ExtraTreesClassifier(random_state=0,n_jobs=-1).fit(X_train,y_train)\n",
    "end_train = time.time()\n",
    "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
    "end_predict = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_predictions)\n",
    "recall = recall_score(y_test, y_predictions, average='weighted')\n",
    "precision = precision_score(y_test, y_predictions, average='weighted')\n",
    "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
    "\n",
    "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
    "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
    "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
    "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
    "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
    "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
    "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
    "model_performance.loc['Extra Trees'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=5,5 \n",
    "sns.set_style(\"white\")\n",
    "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=10,10\n",
    "sns.set_style(\"white\")\n",
    "sns.despine()\n",
    "feat_importances = pd.Series(model.feature_importances_, index=feature_names)\n",
    "feat_importances = feat_importances.groupby(level=0).mean()\n",
    "feat_importances.nlargest(20).plot(kind='barh').invert_yaxis()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "start = time.time()\n",
    "model = RandomForestClassifier(n_estimators = 100,n_jobs=-1,random_state=0,bootstrap=True,).fit(X_train,y_train)\n",
    "end_train = time.time()\n",
    "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
    "end_predict = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_predictions)\n",
    "recall = recall_score(y_test, y_predictions, average='weighted')\n",
    "precision = precision_score(y_test, y_predictions, average='weighted')\n",
    "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
    "\n",
    "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
    "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
    "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
    "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
    "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
    "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
    "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
    "model_performance.loc['Random Forest'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=5,5 \n",
    "sns.set_style(\"white\")\n",
    "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=10,10\n",
    "sns.set_style(\"white\")\n",
    "feat_importances = pd.Series(model.feature_importances_, index=feature_names)\n",
    "feat_importances = feat_importances.groupby(level=0).mean()\n",
    "feat_importances.nlargest(20).plot(kind='barh').invert_yaxis()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "start = time.time()\n",
    "model = GradientBoostingClassifier().fit(X_train,y_train)\n",
    "end_train = time.time()\n",
    "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
    "end_predict = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_predictions)\n",
    "recall = recall_score(y_test, y_predictions, average='weighted')\n",
    "precision = precision_score(y_test, y_predictions, average='weighted')\n",
    "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
    "\n",
    "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
    "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
    "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
    "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
    "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
    "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
    "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
    "model_performance.loc['Gradient Boosting Classifier'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=5,5 \n",
    "sns.set_style(\"white\")\n",
    "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=10,10\n",
    "sns.set_style(\"white\")\n",
    "feat_importances = pd.Series(model.feature_importances_, index=feature_names)\n",
    "feat_importances = feat_importances.groupby(level=0).mean()\n",
    "feat_importances.nlargest(20).plot(kind='barh').invert_yaxis()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "start = time.time()\n",
    "model = MLPClassifier(hidden_layer_sizes = (20,20,), \n",
    "                      activation='relu', \n",
    "                      solver='adam',\n",
    "                      batch_size=2000,\n",
    "                      verbose=0).fit(X_train,y_train)\n",
    "end_train = time.time()\n",
    "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
    "end_predict = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_predictions)\n",
    "recall = recall_score(y_test, y_predictions, average='weighted')\n",
    "precision = precision_score(y_test, y_predictions, average='weighted')\n",
    "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
    "\n",
    "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
    "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
    "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
    "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
    "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
    "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
    "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
    "model_performance.loc['MLP'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=5,5 \n",
    "sns.set_style(\"white\")\n",
    "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3361e_row0_col0, #T_3361e_row0_col1, #T_3361e_row0_col3 {\n",
       "  background-color: #d1493f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3361e_row0_col2 {\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3361e_row0_col4, #T_3361e_row0_col6, #T_3361e_row2_col0, #T_3361e_row2_col1, #T_3361e_row2_col2, #T_3361e_row2_col3, #T_3361e_row3_col5 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3361e_row0_col5, #T_3361e_row1_col0, #T_3361e_row1_col1, #T_3361e_row1_col2, #T_3361e_row1_col3, #T_3361e_row2_col4, #T_3361e_row2_col6 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3361e_row1_col4, #T_3361e_row1_col6 {\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3361e_row1_col5 {\n",
       "  background-color: #efcebd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3361e_row2_col5 {\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3361e_row3_col0, #T_3361e_row3_col1, #T_3361e_row3_col3 {\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3361e_row3_col2 {\n",
       "  background-color: #9fbfff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3361e_row3_col4, #T_3361e_row3_col6 {\n",
       "  background-color: #89acfd;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3361e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3361e_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_3361e_level0_col1\" class=\"col_heading level0 col1\" >Recall</th>\n",
       "      <th id=\"T_3361e_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_3361e_level0_col3\" class=\"col_heading level0 col3\" >F1-Score</th>\n",
       "      <th id=\"T_3361e_level0_col4\" class=\"col_heading level0 col4\" >time to train</th>\n",
       "      <th id=\"T_3361e_level0_col5\" class=\"col_heading level0 col5\" >time to predict</th>\n",
       "      <th id=\"T_3361e_level0_col6\" class=\"col_heading level0 col6\" >total time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3361e_level0_row0\" class=\"row_heading level0 row0\" >Extra Trees</th>\n",
       "      <td id=\"T_3361e_row0_col0\" class=\"data row0 col0\" >97.53%</td>\n",
       "      <td id=\"T_3361e_row0_col1\" class=\"data row0 col1\" >97.53%</td>\n",
       "      <td id=\"T_3361e_row0_col2\" class=\"data row0 col2\" >97.55%</td>\n",
       "      <td id=\"T_3361e_row0_col3\" class=\"data row0 col3\" >97.53%</td>\n",
       "      <td id=\"T_3361e_row0_col4\" class=\"data row0 col4\" >0.6</td>\n",
       "      <td id=\"T_3361e_row0_col5\" class=\"data row0 col5\" >0.1</td>\n",
       "      <td id=\"T_3361e_row0_col6\" class=\"data row0 col6\" >0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3361e_level0_row1\" class=\"row_heading level0 row1\" >Random Forest</th>\n",
       "      <td id=\"T_3361e_row1_col0\" class=\"data row1 col0\" >97.68%</td>\n",
       "      <td id=\"T_3361e_row1_col1\" class=\"data row1 col1\" >97.68%</td>\n",
       "      <td id=\"T_3361e_row1_col2\" class=\"data row1 col2\" >97.69%</td>\n",
       "      <td id=\"T_3361e_row1_col3\" class=\"data row1 col3\" >97.68%</td>\n",
       "      <td id=\"T_3361e_row1_col4\" class=\"data row1 col4\" >1.5</td>\n",
       "      <td id=\"T_3361e_row1_col5\" class=\"data row1 col5\" >0.0</td>\n",
       "      <td id=\"T_3361e_row1_col6\" class=\"data row1 col6\" >1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3361e_level0_row2\" class=\"row_heading level0 row2\" >Gradient Boosting Classifier</th>\n",
       "      <td id=\"T_3361e_row2_col0\" class=\"data row2 col0\" >95.85%</td>\n",
       "      <td id=\"T_3361e_row2_col1\" class=\"data row2 col1\" >95.85%</td>\n",
       "      <td id=\"T_3361e_row2_col2\" class=\"data row2 col2\" >95.86%</td>\n",
       "      <td id=\"T_3361e_row2_col3\" class=\"data row2 col3\" >95.85%</td>\n",
       "      <td id=\"T_3361e_row2_col4\" class=\"data row2 col4\" >41.2</td>\n",
       "      <td id=\"T_3361e_row2_col5\" class=\"data row2 col5\" >0.0</td>\n",
       "      <td id=\"T_3361e_row2_col6\" class=\"data row2 col6\" >41.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3361e_level0_row3\" class=\"row_heading level0 row3\" >MLP</th>\n",
       "      <td id=\"T_3361e_row3_col0\" class=\"data row3 col0\" >96.39%</td>\n",
       "      <td id=\"T_3361e_row3_col1\" class=\"data row3 col1\" >96.39%</td>\n",
       "      <td id=\"T_3361e_row3_col2\" class=\"data row3 col2\" >96.41%</td>\n",
       "      <td id=\"T_3361e_row3_col3\" class=\"data row3 col3\" >96.40%</td>\n",
       "      <td id=\"T_3361e_row3_col4\" class=\"data row3 col4\" >10.4</td>\n",
       "      <td id=\"T_3361e_row3_col5\" class=\"data row3 col5\" >0.0</td>\n",
       "      <td id=\"T_3361e_row3_col6\" class=\"data row3 col6\" >10.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2269c33cf40>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance.style.background_gradient(cmap='coolwarm').format({'Accuracy': '{:.2%}',\n",
    "                                                                     'Precision': '{:.2%}',\n",
    "                                                                     'Recall': '{:.2%}',\n",
    "                                                                     'F1-Score': '{:.2%}',\n",
    "                                                                     'time to train':'{:.1f}',\n",
    "                                                                     'time to predict':'{:.1f}',\n",
    "                                                                     'total time':'{:.1f}',\n",
    "                                                                     })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network MLP (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU\n",
    "from keras import metrics\n",
    "# !pip install keras-metrics #It doesn't come with Google Colab\n",
    "import keras_metrics as km #when compiling\n",
    "import keras\n",
    "import numpy as np\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "33/33 - 1s - loss: 2.3068 - accuracy: 0.3454 - f1_m: 4.0136e-04 - precision_m: 0.0303 - recall_m: 2.0202e-04 - 719ms/epoch - 22ms/step\n",
      "Epoch 2/200\n",
      "33/33 - 0s - loss: 1.2158 - accuracy: 0.7445 - f1_m: 0.5334 - precision_m: 0.7181 - recall_m: 0.4863 - 57ms/epoch - 2ms/step\n",
      "Epoch 3/200\n",
      "33/33 - 0s - loss: 0.5448 - accuracy: 0.7727 - f1_m: 0.6589 - precision_m: 0.5654 - recall_m: 0.7970 - 55ms/epoch - 2ms/step\n",
      "Epoch 4/200\n",
      "33/33 - 0s - loss: 0.3858 - accuracy: 0.8096 - f1_m: 0.6982 - precision_m: 0.5485 - recall_m: 0.9606 - 57ms/epoch - 2ms/step\n",
      "Epoch 5/200\n",
      "33/33 - 0s - loss: 0.3346 - accuracy: 0.8389 - f1_m: 0.7035 - precision_m: 0.5487 - recall_m: 0.9805 - 60ms/epoch - 2ms/step\n",
      "Epoch 6/200\n",
      "33/33 - 0s - loss: 0.3009 - accuracy: 0.8590 - f1_m: 0.7085 - precision_m: 0.5506 - recall_m: 0.9936 - 59ms/epoch - 2ms/step\n",
      "Epoch 7/200\n",
      "33/33 - 0s - loss: 0.2746 - accuracy: 0.8690 - f1_m: 0.7093 - precision_m: 0.5507 - recall_m: 0.9966 - 56ms/epoch - 2ms/step\n",
      "Epoch 8/200\n",
      "33/33 - 0s - loss: 0.2535 - accuracy: 0.8768 - f1_m: 0.7098 - precision_m: 0.5507 - recall_m: 0.9984 - 56ms/epoch - 2ms/step\n",
      "Epoch 9/200\n",
      "33/33 - 0s - loss: 0.2374 - accuracy: 0.8829 - f1_m: 0.7099 - precision_m: 0.5507 - recall_m: 0.9988 - 55ms/epoch - 2ms/step\n",
      "Epoch 10/200\n",
      "33/33 - 0s - loss: 0.2235 - accuracy: 0.8937 - f1_m: 0.7100 - precision_m: 0.5507 - recall_m: 0.9993 - 56ms/epoch - 2ms/step\n",
      "Epoch 11/200\n",
      "33/33 - 0s - loss: 0.2128 - accuracy: 0.9027 - f1_m: 0.7101 - precision_m: 0.5508 - recall_m: 0.9991 - 57ms/epoch - 2ms/step\n",
      "Epoch 12/200\n",
      "33/33 - 0s - loss: 0.2047 - accuracy: 0.9077 - f1_m: 0.7101 - precision_m: 0.5508 - recall_m: 0.9995 - 57ms/epoch - 2ms/step\n",
      "Epoch 13/200\n",
      "33/33 - 0s - loss: 0.1984 - accuracy: 0.9119 - f1_m: 0.7101 - precision_m: 0.5507 - recall_m: 0.9996 - 55ms/epoch - 2ms/step\n",
      "Epoch 14/200\n",
      "33/33 - 0s - loss: 0.1932 - accuracy: 0.9153 - f1_m: 0.7100 - precision_m: 0.5506 - recall_m: 0.9995 - 56ms/epoch - 2ms/step\n",
      "Epoch 15/200\n",
      "33/33 - 0s - loss: 0.1889 - accuracy: 0.9184 - f1_m: 0.7101 - precision_m: 0.5507 - recall_m: 0.9997 - 54ms/epoch - 2ms/step\n",
      "Epoch 16/200\n",
      "33/33 - 0s - loss: 0.1849 - accuracy: 0.9211 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9997 - 57ms/epoch - 2ms/step\n",
      "Epoch 17/200\n",
      "33/33 - 0s - loss: 0.1812 - accuracy: 0.9235 - f1_m: 0.7100 - precision_m: 0.5506 - recall_m: 0.9996 - 53ms/epoch - 2ms/step\n",
      "Epoch 18/200\n",
      "33/33 - 0s - loss: 0.1779 - accuracy: 0.9252 - f1_m: 0.7100 - precision_m: 0.5506 - recall_m: 0.9996 - 56ms/epoch - 2ms/step\n",
      "Epoch 19/200\n",
      "33/33 - 0s - loss: 0.1743 - accuracy: 0.9271 - f1_m: 0.7102 - precision_m: 0.5507 - recall_m: 0.9998 - 54ms/epoch - 2ms/step\n",
      "Epoch 20/200\n",
      "33/33 - 0s - loss: 0.1713 - accuracy: 0.9295 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9998 - 55ms/epoch - 2ms/step\n",
      "Epoch 21/200\n",
      "33/33 - 0s - loss: 0.1680 - accuracy: 0.9315 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9998 - 55ms/epoch - 2ms/step\n",
      "Epoch 22/200\n",
      "33/33 - 0s - loss: 0.1651 - accuracy: 0.9341 - f1_m: 0.7100 - precision_m: 0.5506 - recall_m: 0.9998 - 56ms/epoch - 2ms/step\n",
      "Epoch 23/200\n",
      "33/33 - 0s - loss: 0.1627 - accuracy: 0.9352 - f1_m: 0.7100 - precision_m: 0.5506 - recall_m: 0.9997 - 55ms/epoch - 2ms/step\n",
      "Epoch 24/200\n",
      "33/33 - 0s - loss: 0.1599 - accuracy: 0.9371 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 54ms/epoch - 2ms/step\n",
      "Epoch 25/200\n",
      "33/33 - 0s - loss: 0.1577 - accuracy: 0.9381 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9998 - 54ms/epoch - 2ms/step\n",
      "Epoch 26/200\n",
      "33/33 - 0s - loss: 0.1554 - accuracy: 0.9401 - f1_m: 0.7101 - precision_m: 0.5507 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 27/200\n",
      "33/33 - 0s - loss: 0.1534 - accuracy: 0.9413 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 55ms/epoch - 2ms/step\n",
      "Epoch 28/200\n",
      "33/33 - 0s - loss: 0.1512 - accuracy: 0.9422 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9998 - 56ms/epoch - 2ms/step\n",
      "Epoch 29/200\n",
      "33/33 - 0s - loss: 0.1496 - accuracy: 0.9438 - f1_m: 0.7100 - precision_m: 0.5506 - recall_m: 0.9999 - 55ms/epoch - 2ms/step\n",
      "Epoch 30/200\n",
      "33/33 - 0s - loss: 0.1477 - accuracy: 0.9442 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 55ms/epoch - 2ms/step\n",
      "Epoch 31/200\n",
      "33/33 - 0s - loss: 0.1463 - accuracy: 0.9457 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 55ms/epoch - 2ms/step\n",
      "Epoch 32/200\n",
      "33/33 - 0s - loss: 0.1447 - accuracy: 0.9460 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 33/200\n",
      "33/33 - 0s - loss: 0.1436 - accuracy: 0.9459 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 34/200\n",
      "33/33 - 0s - loss: 0.1423 - accuracy: 0.9472 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 54ms/epoch - 2ms/step\n",
      "Epoch 35/200\n",
      "33/33 - 0s - loss: 0.1412 - accuracy: 0.9468 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 36/200\n",
      "33/33 - 0s - loss: 0.1382 - accuracy: 0.9489 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 63ms/epoch - 2ms/step\n",
      "Epoch 37/200\n",
      "33/33 - 0s - loss: 0.1371 - accuracy: 0.9498 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 57ms/epoch - 2ms/step\n",
      "Epoch 38/200\n",
      "33/33 - 0s - loss: 0.1360 - accuracy: 0.9499 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 54ms/epoch - 2ms/step\n",
      "Epoch 39/200\n",
      "33/33 - 0s - loss: 0.1345 - accuracy: 0.9505 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 56ms/epoch - 2ms/step\n",
      "Epoch 40/200\n",
      "33/33 - 0s - loss: 0.1338 - accuracy: 0.9508 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 57ms/epoch - 2ms/step\n",
      "Epoch 41/200\n",
      "33/33 - 0s - loss: 0.1329 - accuracy: 0.9512 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 42/200\n",
      "33/33 - 0s - loss: 0.1319 - accuracy: 0.9520 - f1_m: 0.7101 - precision_m: 0.5507 - recall_m: 0.9999 - 58ms/epoch - 2ms/step\n",
      "Epoch 43/200\n",
      "33/33 - 0s - loss: 0.1307 - accuracy: 0.9524 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 55ms/epoch - 2ms/step\n",
      "Epoch 44/200\n",
      "33/33 - 0s - loss: 0.1296 - accuracy: 0.9532 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 45/200\n",
      "33/33 - 0s - loss: 0.1290 - accuracy: 0.9527 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 55ms/epoch - 2ms/step\n",
      "Epoch 46/200\n",
      "33/33 - 0s - loss: 0.1283 - accuracy: 0.9531 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 47/200\n",
      "33/33 - 0s - loss: 0.1279 - accuracy: 0.9534 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 56ms/epoch - 2ms/step\n",
      "Epoch 48/200\n",
      "33/33 - 0s - loss: 0.1277 - accuracy: 0.9530 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 55ms/epoch - 2ms/step\n",
      "Epoch 49/200\n",
      "33/33 - 0s - loss: 0.1264 - accuracy: 0.9536 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 50/200\n",
      "33/33 - 0s - loss: 0.1262 - accuracy: 0.9534 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 51/200\n",
      "33/33 - 0s - loss: 0.1247 - accuracy: 0.9540 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 52/200\n",
      "33/33 - 0s - loss: 0.1239 - accuracy: 0.9551 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 53/200\n",
      "33/33 - 0s - loss: 0.1233 - accuracy: 0.9540 - f1_m: 0.7102 - precision_m: 0.5507 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 54/200\n",
      "33/33 - 0s - loss: 0.1228 - accuracy: 0.9547 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 55ms/epoch - 2ms/step\n",
      "Epoch 55/200\n",
      "33/33 - 0s - loss: 0.1223 - accuracy: 0.9548 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 56/200\n",
      "33/33 - 0s - loss: 0.1218 - accuracy: 0.9550 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 57/200\n",
      "33/33 - 0s - loss: 0.1210 - accuracy: 0.9556 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 58/200\n",
      "33/33 - 0s - loss: 0.1208 - accuracy: 0.9552 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 56ms/epoch - 2ms/step\n",
      "Epoch 59/200\n",
      "33/33 - 0s - loss: 0.1201 - accuracy: 0.9554 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 55ms/epoch - 2ms/step\n",
      "Epoch 60/200\n",
      "33/33 - 0s - loss: 0.1191 - accuracy: 0.9562 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 61/200\n",
      "33/33 - 0s - loss: 0.1185 - accuracy: 0.9559 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 62/200\n",
      "33/33 - 0s - loss: 0.1183 - accuracy: 0.9562 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 63/200\n",
      "33/33 - 0s - loss: 0.1176 - accuracy: 0.9556 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 64/200\n",
      "33/33 - 0s - loss: 0.1168 - accuracy: 0.9563 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 65/200\n",
      "33/33 - 0s - loss: 0.1165 - accuracy: 0.9560 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 60ms/epoch - 2ms/step\n",
      "Epoch 66/200\n",
      "33/33 - 0s - loss: 0.1161 - accuracy: 0.9562 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 67/200\n",
      "33/33 - 0s - loss: 0.1158 - accuracy: 0.9568 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 68/200\n",
      "33/33 - 0s - loss: 0.1169 - accuracy: 0.9552 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 69/200\n",
      "33/33 - 0s - loss: 0.1150 - accuracy: 0.9559 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 70/200\n",
      "33/33 - 0s - loss: 0.1138 - accuracy: 0.9564 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 71/200\n",
      "33/33 - 0s - loss: 0.1133 - accuracy: 0.9575 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 72/200\n",
      "33/33 - 0s - loss: 0.1127 - accuracy: 0.9571 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 73/200\n",
      "33/33 - 0s - loss: 0.1128 - accuracy: 0.9576 - f1_m: 0.7100 - precision_m: 0.5505 - recall_m: 0.9999 - 57ms/epoch - 2ms/step\n",
      "Epoch 74/200\n",
      "33/33 - 0s - loss: 0.1123 - accuracy: 0.9574 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 73ms/epoch - 2ms/step\n",
      "Epoch 75/200\n",
      "33/33 - 0s - loss: 0.1123 - accuracy: 0.9574 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 55ms/epoch - 2ms/step\n",
      "Epoch 76/200\n",
      "33/33 - 0s - loss: 0.1109 - accuracy: 0.9583 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 77/200\n",
      "33/33 - 0s - loss: 0.1104 - accuracy: 0.9581 - f1_m: 0.7102 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 78/200\n",
      "33/33 - 0s - loss: 0.1102 - accuracy: 0.9582 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 79/200\n",
      "33/33 - 0s - loss: 0.1118 - accuracy: 0.9570 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 55ms/epoch - 2ms/step\n",
      "Epoch 80/200\n",
      "33/33 - 0s - loss: 0.1111 - accuracy: 0.9573 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 57ms/epoch - 2ms/step\n",
      "Epoch 81/200\n",
      "33/33 - 0s - loss: 0.1091 - accuracy: 0.9587 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 82/200\n",
      "33/33 - 0s - loss: 0.1092 - accuracy: 0.9582 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 83/200\n",
      "33/33 - 0s - loss: 0.1087 - accuracy: 0.9586 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 84/200\n",
      "33/33 - 0s - loss: 0.1079 - accuracy: 0.9590 - f1_m: 0.7102 - precision_m: 0.5507 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 85/200\n",
      "33/33 - 0s - loss: 0.1082 - accuracy: 0.9590 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 86/200\n",
      "33/33 - 0s - loss: 0.1078 - accuracy: 0.9590 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 87/200\n",
      "33/33 - 0s - loss: 0.1071 - accuracy: 0.9590 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 88/200\n",
      "33/33 - 0s - loss: 0.1064 - accuracy: 0.9593 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 89/200\n",
      "33/33 - 0s - loss: 0.1063 - accuracy: 0.9594 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 90/200\n",
      "33/33 - 0s - loss: 0.1067 - accuracy: 0.9590 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 91/200\n",
      "33/33 - 0s - loss: 0.1059 - accuracy: 0.9595 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 92/200\n",
      "33/33 - 0s - loss: 0.1051 - accuracy: 0.9601 - f1_m: 0.7102 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 93/200\n",
      "33/33 - 0s - loss: 0.1053 - accuracy: 0.9593 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 94/200\n",
      "33/33 - 0s - loss: 0.1048 - accuracy: 0.9598 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 53ms/epoch - 2ms/step\n",
      "Epoch 95/200\n",
      "33/33 - 0s - loss: 0.1054 - accuracy: 0.9591 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 96/200\n",
      "33/33 - 0s - loss: 0.1046 - accuracy: 0.9600 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 0.9999 - 55ms/epoch - 2ms/step\n",
      "Epoch 97/200\n",
      "33/33 - 0s - loss: 0.1044 - accuracy: 0.9602 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 98/200\n",
      "33/33 - 0s - loss: 0.1044 - accuracy: 0.9602 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 99/200\n",
      "33/33 - 0s - loss: 0.1035 - accuracy: 0.9601 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 100/200\n",
      "33/33 - 0s - loss: 0.1029 - accuracy: 0.9608 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 101/200\n",
      "33/33 - 0s - loss: 0.1021 - accuracy: 0.9610 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 53ms/epoch - 2ms/step\n",
      "Epoch 102/200\n",
      "33/33 - 0s - loss: 0.1025 - accuracy: 0.9604 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 103/200\n",
      "33/33 - 0s - loss: 0.1026 - accuracy: 0.9601 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 104/200\n",
      "33/33 - 0s - loss: 0.1034 - accuracy: 0.9602 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 105/200\n",
      "33/33 - 0s - loss: 0.1019 - accuracy: 0.9609 - f1_m: 0.7102 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 106/200\n",
      "33/33 - 0s - loss: 0.1007 - accuracy: 0.9616 - f1_m: 0.7102 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 107/200\n",
      "33/33 - 0s - loss: 0.1008 - accuracy: 0.9613 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 108/200\n",
      "33/33 - 0s - loss: 0.1006 - accuracy: 0.9615 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 109/200\n",
      "33/33 - 0s - loss: 0.0999 - accuracy: 0.9622 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 110/200\n",
      "33/33 - 0s - loss: 0.1001 - accuracy: 0.9620 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 111/200\n",
      "33/33 - 0s - loss: 0.0995 - accuracy: 0.9624 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 112/200\n",
      "33/33 - 0s - loss: 0.0995 - accuracy: 0.9618 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 69ms/epoch - 2ms/step\n",
      "Epoch 113/200\n",
      "33/33 - 0s - loss: 0.0988 - accuracy: 0.9627 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 114/200\n",
      "33/33 - 0s - loss: 0.0988 - accuracy: 0.9624 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 115/200\n",
      "33/33 - 0s - loss: 0.0989 - accuracy: 0.9619 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 116/200\n",
      "33/33 - 0s - loss: 0.0983 - accuracy: 0.9625 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 117/200\n",
      "33/33 - 0s - loss: 0.0982 - accuracy: 0.9623 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 57ms/epoch - 2ms/step\n",
      "Epoch 118/200\n",
      "33/33 - 0s - loss: 0.0986 - accuracy: 0.9616 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 119/200\n",
      "33/33 - 0s - loss: 0.0978 - accuracy: 0.9620 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 120/200\n",
      "33/33 - 0s - loss: 0.0987 - accuracy: 0.9619 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 121/200\n",
      "33/33 - 0s - loss: 0.0986 - accuracy: 0.9616 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 122/200\n",
      "33/33 - 0s - loss: 0.0979 - accuracy: 0.9622 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 123/200\n",
      "33/33 - 0s - loss: 0.0967 - accuracy: 0.9625 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 124/200\n",
      "33/33 - 0s - loss: 0.0973 - accuracy: 0.9623 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 125/200\n",
      "33/33 - 0s - loss: 0.0973 - accuracy: 0.9623 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 126/200\n",
      "33/33 - 0s - loss: 0.0967 - accuracy: 0.9629 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 127/200\n",
      "33/33 - 0s - loss: 0.0966 - accuracy: 0.9627 - f1_m: 0.7100 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 128/200\n",
      "33/33 - 0s - loss: 0.0959 - accuracy: 0.9634 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 129/200\n",
      "33/33 - 0s - loss: 0.0959 - accuracy: 0.9633 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 130/200\n",
      "33/33 - 0s - loss: 0.0956 - accuracy: 0.9635 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 131/200\n",
      "33/33 - 0s - loss: 0.0971 - accuracy: 0.9630 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 132/200\n",
      "33/33 - 0s - loss: 0.0953 - accuracy: 0.9636 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 53ms/epoch - 2ms/step\n",
      "Epoch 133/200\n",
      "33/33 - 0s - loss: 0.0950 - accuracy: 0.9634 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 134/200\n",
      "33/33 - 0s - loss: 0.0952 - accuracy: 0.9634 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 135/200\n",
      "33/33 - 0s - loss: 0.0956 - accuracy: 0.9629 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 136/200\n",
      "33/33 - 0s - loss: 0.0950 - accuracy: 0.9635 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 137/200\n",
      "33/33 - 0s - loss: 0.0944 - accuracy: 0.9642 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 138/200\n",
      "33/33 - 0s - loss: 0.0952 - accuracy: 0.9632 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 139/200\n",
      "33/33 - 0s - loss: 0.0951 - accuracy: 0.9631 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 140/200\n",
      "33/33 - 0s - loss: 0.0947 - accuracy: 0.9634 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 141/200\n",
      "33/33 - 0s - loss: 0.0941 - accuracy: 0.9642 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 142/200\n",
      "33/33 - 0s - loss: 0.0937 - accuracy: 0.9635 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 143/200\n",
      "33/33 - 0s - loss: 0.0939 - accuracy: 0.9640 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 144/200\n",
      "33/33 - 0s - loss: 0.0933 - accuracy: 0.9646 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 145/200\n",
      "33/33 - 0s - loss: 0.0931 - accuracy: 0.9645 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 146/200\n",
      "33/33 - 0s - loss: 0.0929 - accuracy: 0.9647 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 66ms/epoch - 2ms/step\n",
      "Epoch 147/200\n",
      "33/33 - 0s - loss: 0.0930 - accuracy: 0.9643 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 57ms/epoch - 2ms/step\n",
      "Epoch 148/200\n",
      "33/33 - 0s - loss: 0.0942 - accuracy: 0.9637 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 57ms/epoch - 2ms/step\n",
      "Epoch 149/200\n",
      "33/33 - 0s - loss: 0.0932 - accuracy: 0.9647 - f1_m: 0.7100 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 150/200\n",
      "33/33 - 0s - loss: 0.0937 - accuracy: 0.9638 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 151/200\n",
      "33/33 - 0s - loss: 0.0931 - accuracy: 0.9640 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 152/200\n",
      "33/33 - 0s - loss: 0.0929 - accuracy: 0.9644 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 153/200\n",
      "33/33 - 0s - loss: 0.0930 - accuracy: 0.9638 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 154/200\n",
      "33/33 - 0s - loss: 0.0927 - accuracy: 0.9649 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 58ms/epoch - 2ms/step\n",
      "Epoch 155/200\n",
      "33/33 - 0s - loss: 0.0921 - accuracy: 0.9653 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 57ms/epoch - 2ms/step\n",
      "Epoch 156/200\n",
      "33/33 - 0s - loss: 0.0923 - accuracy: 0.9651 - f1_m: 0.7102 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 157/200\n",
      "33/33 - 0s - loss: 0.0930 - accuracy: 0.9645 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 158/200\n",
      "33/33 - 0s - loss: 0.0937 - accuracy: 0.9637 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 159/200\n",
      "33/33 - 0s - loss: 0.0919 - accuracy: 0.9646 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 160/200\n",
      "33/33 - 0s - loss: 0.0915 - accuracy: 0.9657 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 161/200\n",
      "33/33 - 0s - loss: 0.0913 - accuracy: 0.9652 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 162/200\n",
      "33/33 - 0s - loss: 0.0918 - accuracy: 0.9647 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 57ms/epoch - 2ms/step\n",
      "Epoch 163/200\n",
      "33/33 - 0s - loss: 0.0919 - accuracy: 0.9645 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 164/200\n",
      "33/33 - 0s - loss: 0.0918 - accuracy: 0.9650 - f1_m: 0.7101 - precision_m: 0.5505 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 165/200\n",
      "33/33 - 0s - loss: 0.0908 - accuracy: 0.9655 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 166/200\n",
      "33/33 - 0s - loss: 0.0913 - accuracy: 0.9653 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 167/200\n",
      "33/33 - 0s - loss: 0.0915 - accuracy: 0.9647 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 168/200\n",
      "33/33 - 0s - loss: 0.0915 - accuracy: 0.9649 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 169/200\n",
      "33/33 - 0s - loss: 0.0918 - accuracy: 0.9650 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 170/200\n",
      "33/33 - 0s - loss: 0.0912 - accuracy: 0.9652 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 171/200\n",
      "33/33 - 0s - loss: 0.0922 - accuracy: 0.9643 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 172/200\n",
      "33/33 - 0s - loss: 0.0914 - accuracy: 0.9653 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 173/200\n",
      "33/33 - 0s - loss: 0.0908 - accuracy: 0.9652 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 174/200\n",
      "33/33 - 0s - loss: 0.0908 - accuracy: 0.9656 - f1_m: 0.7100 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 175/200\n",
      "33/33 - 0s - loss: 0.0912 - accuracy: 0.9653 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 176/200\n",
      "33/33 - 0s - loss: 0.0900 - accuracy: 0.9660 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 177/200\n",
      "33/33 - 0s - loss: 0.0904 - accuracy: 0.9653 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 178/200\n",
      "33/33 - 0s - loss: 0.0910 - accuracy: 0.9650 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 179/200\n",
      "33/33 - 0s - loss: 0.0914 - accuracy: 0.9649 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 70ms/epoch - 2ms/step\n",
      "Epoch 180/200\n",
      "33/33 - 0s - loss: 0.0898 - accuracy: 0.9656 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 181/200\n",
      "33/33 - 0s - loss: 0.0901 - accuracy: 0.9652 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 182/200\n",
      "33/33 - 0s - loss: 0.0903 - accuracy: 0.9656 - f1_m: 0.7102 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 183/200\n",
      "33/33 - 0s - loss: 0.0900 - accuracy: 0.9656 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 184/200\n",
      "33/33 - 0s - loss: 0.0898 - accuracy: 0.9654 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 185/200\n",
      "33/33 - 0s - loss: 0.0908 - accuracy: 0.9648 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 186/200\n",
      "33/33 - 0s - loss: 0.0895 - accuracy: 0.9660 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 187/200\n",
      "33/33 - 0s - loss: 0.0902 - accuracy: 0.9650 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 188/200\n",
      "33/33 - 0s - loss: 0.0896 - accuracy: 0.9661 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 189/200\n",
      "33/33 - 0s - loss: 0.0898 - accuracy: 0.9662 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 190/200\n",
      "33/33 - 0s - loss: 0.0894 - accuracy: 0.9657 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 191/200\n",
      "33/33 - 0s - loss: 0.0890 - accuracy: 0.9658 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 192/200\n",
      "33/33 - 0s - loss: 0.0900 - accuracy: 0.9655 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 57ms/epoch - 2ms/step\n",
      "Epoch 193/200\n",
      "33/33 - 0s - loss: 0.0896 - accuracy: 0.9654 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 194/200\n",
      "33/33 - 0s - loss: 0.0886 - accuracy: 0.9663 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 195/200\n",
      "33/33 - 0s - loss: 0.0890 - accuracy: 0.9659 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 54ms/epoch - 2ms/step\n",
      "Epoch 196/200\n",
      "33/33 - 0s - loss: 0.0887 - accuracy: 0.9662 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 197/200\n",
      "33/33 - 0s - loss: 0.0900 - accuracy: 0.9654 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n",
      "Epoch 198/200\n",
      "33/33 - 0s - loss: 0.0911 - accuracy: 0.9648 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 199/200\n",
      "33/33 - 0s - loss: 0.0897 - accuracy: 0.9657 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 55ms/epoch - 2ms/step\n",
      "Epoch 200/200\n",
      "33/33 - 0s - loss: 0.0883 - accuracy: 0.9665 - f1_m: 0.7101 - precision_m: 0.5506 - recall_m: 1.0000 - 56ms/epoch - 2ms/step\n"
     ]
    }
   ],
   "source": [
    "#Build the feed forward neural network model\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=56, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(20, activation='softmax')) #for multiclass classification\n",
    "    #Compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "#institate the model\n",
    "model = build_model()\n",
    "\n",
    "#fit the model\n",
    "start = time.time()\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=2000,verbose=2)\n",
    "end_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 1s 757us/step - loss: 0.0928 - accuracy: 0.9625 - f1_m: 0.7060 - precision_m: 0.5505 - recall_m: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the neural network\n",
    "loss, accuracy, f1s, precision, recall = model.evaluate(X_test, y_test)\n",
    "end_predict = time.time()\n",
    "model_performance.loc['MLP (Keras)'] = [accuracy, accuracy, accuracy, accuracy,end_train-start,end_predict-end_train,end_predict-start]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65865\n",
      "Epoch 1/200\n",
      "33/33 - 2s - loss: 2.0964 - accuracy: 0.5346 - 2s/epoch - 69ms/step\n",
      "Epoch 2/200\n",
      "33/33 - 0s - loss: 1.4500 - accuracy: 0.7005 - 121ms/epoch - 4ms/step\n",
      "Epoch 3/200\n",
      "33/33 - 0s - loss: 0.8300 - accuracy: 0.7059 - 116ms/epoch - 4ms/step\n",
      "Epoch 4/200\n",
      "33/33 - 0s - loss: 0.5948 - accuracy: 0.7483 - 121ms/epoch - 4ms/step\n",
      "Epoch 5/200\n",
      "33/33 - 0s - loss: 0.5024 - accuracy: 0.7642 - 118ms/epoch - 4ms/step\n",
      "Epoch 6/200\n",
      "33/33 - 0s - loss: 0.4315 - accuracy: 0.7902 - 118ms/epoch - 4ms/step\n",
      "Epoch 7/200\n",
      "33/33 - 0s - loss: 0.3675 - accuracy: 0.8442 - 119ms/epoch - 4ms/step\n",
      "Epoch 8/200\n",
      "33/33 - 0s - loss: 0.3140 - accuracy: 0.8659 - 118ms/epoch - 4ms/step\n",
      "Epoch 9/200\n",
      "33/33 - 0s - loss: 0.2720 - accuracy: 0.8788 - 120ms/epoch - 4ms/step\n",
      "Epoch 10/200\n",
      "33/33 - 0s - loss: 0.2432 - accuracy: 0.8926 - 123ms/epoch - 4ms/step\n",
      "Epoch 11/200\n",
      "33/33 - 0s - loss: 0.2247 - accuracy: 0.9015 - 125ms/epoch - 4ms/step\n",
      "Epoch 12/200\n",
      "33/33 - 0s - loss: 0.2114 - accuracy: 0.9081 - 119ms/epoch - 4ms/step\n",
      "Epoch 13/200\n",
      "33/33 - 0s - loss: 0.2014 - accuracy: 0.9129 - 121ms/epoch - 4ms/step\n",
      "Epoch 14/200\n",
      "33/33 - 0s - loss: 0.1927 - accuracy: 0.9163 - 119ms/epoch - 4ms/step\n",
      "Epoch 15/200\n",
      "33/33 - 0s - loss: 0.1852 - accuracy: 0.9211 - 119ms/epoch - 4ms/step\n",
      "Epoch 16/200\n",
      "33/33 - 0s - loss: 0.1781 - accuracy: 0.9249 - 130ms/epoch - 4ms/step\n",
      "Epoch 17/200\n",
      "33/33 - 0s - loss: 0.1708 - accuracy: 0.9300 - 120ms/epoch - 4ms/step\n",
      "Epoch 18/200\n",
      "33/33 - 0s - loss: 0.1643 - accuracy: 0.9338 - 128ms/epoch - 4ms/step\n",
      "Epoch 19/200\n",
      "33/33 - 0s - loss: 0.1583 - accuracy: 0.9380 - 123ms/epoch - 4ms/step\n",
      "Epoch 20/200\n",
      "33/33 - 0s - loss: 0.1524 - accuracy: 0.9408 - 121ms/epoch - 4ms/step\n",
      "Epoch 21/200\n",
      "33/33 - 0s - loss: 0.1476 - accuracy: 0.9428 - 121ms/epoch - 4ms/step\n",
      "Epoch 22/200\n",
      "33/33 - 0s - loss: 0.1430 - accuracy: 0.9461 - 122ms/epoch - 4ms/step\n",
      "Epoch 23/200\n",
      "33/33 - 0s - loss: 0.1392 - accuracy: 0.9481 - 121ms/epoch - 4ms/step\n",
      "Epoch 24/200\n",
      "33/33 - 0s - loss: 0.1368 - accuracy: 0.9493 - 121ms/epoch - 4ms/step\n",
      "Epoch 25/200\n",
      "33/33 - 0s - loss: 0.1341 - accuracy: 0.9507 - 123ms/epoch - 4ms/step\n",
      "Epoch 26/200\n",
      "33/33 - 0s - loss: 0.1315 - accuracy: 0.9519 - 118ms/epoch - 4ms/step\n",
      "Epoch 27/200\n",
      "33/33 - 0s - loss: 0.1297 - accuracy: 0.9531 - 118ms/epoch - 4ms/step\n",
      "Epoch 28/200\n",
      "33/33 - 0s - loss: 0.1275 - accuracy: 0.9540 - 122ms/epoch - 4ms/step\n",
      "Epoch 29/200\n",
      "33/33 - 0s - loss: 0.1261 - accuracy: 0.9546 - 121ms/epoch - 4ms/step\n",
      "Epoch 30/200\n",
      "33/33 - 0s - loss: 0.1248 - accuracy: 0.9554 - 127ms/epoch - 4ms/step\n",
      "Epoch 31/200\n",
      "33/33 - 0s - loss: 0.1238 - accuracy: 0.9553 - 129ms/epoch - 4ms/step\n",
      "Epoch 32/200\n",
      "33/33 - 0s - loss: 0.1224 - accuracy: 0.9558 - 128ms/epoch - 4ms/step\n",
      "Epoch 33/200\n",
      "33/33 - 0s - loss: 0.1208 - accuracy: 0.9561 - 125ms/epoch - 4ms/step\n",
      "Epoch 34/200\n",
      "33/33 - 0s - loss: 0.1198 - accuracy: 0.9568 - 119ms/epoch - 4ms/step\n",
      "Epoch 35/200\n",
      "33/33 - 0s - loss: 0.1197 - accuracy: 0.9562 - 127ms/epoch - 4ms/step\n",
      "Epoch 36/200\n",
      "33/33 - 0s - loss: 0.1181 - accuracy: 0.9570 - 124ms/epoch - 4ms/step\n",
      "Epoch 37/200\n",
      "33/33 - 0s - loss: 0.1171 - accuracy: 0.9577 - 116ms/epoch - 4ms/step\n",
      "Epoch 38/200\n",
      "33/33 - 0s - loss: 0.1160 - accuracy: 0.9578 - 118ms/epoch - 4ms/step\n",
      "Epoch 39/200\n",
      "33/33 - 0s - loss: 0.1154 - accuracy: 0.9572 - 119ms/epoch - 4ms/step\n",
      "Epoch 40/200\n",
      "33/33 - 0s - loss: 0.1144 - accuracy: 0.9578 - 116ms/epoch - 4ms/step\n",
      "Epoch 41/200\n",
      "33/33 - 0s - loss: 0.1139 - accuracy: 0.9581 - 130ms/epoch - 4ms/step\n",
      "Epoch 42/200\n",
      "33/33 - 0s - loss: 0.1133 - accuracy: 0.9583 - 115ms/epoch - 3ms/step\n",
      "Epoch 43/200\n",
      "33/33 - 0s - loss: 0.1129 - accuracy: 0.9584 - 120ms/epoch - 4ms/step\n",
      "Epoch 44/200\n",
      "33/33 - 0s - loss: 0.1120 - accuracy: 0.9586 - 121ms/epoch - 4ms/step\n",
      "Epoch 45/200\n",
      "33/33 - 0s - loss: 0.1110 - accuracy: 0.9593 - 123ms/epoch - 4ms/step\n",
      "Epoch 46/200\n",
      "33/33 - 0s - loss: 0.1105 - accuracy: 0.9591 - 123ms/epoch - 4ms/step\n",
      "Epoch 47/200\n",
      "33/33 - 0s - loss: 0.1099 - accuracy: 0.9592 - 118ms/epoch - 4ms/step\n",
      "Epoch 48/200\n",
      "33/33 - 0s - loss: 0.1096 - accuracy: 0.9593 - 117ms/epoch - 4ms/step\n",
      "Epoch 49/200\n",
      "33/33 - 0s - loss: 0.1086 - accuracy: 0.9596 - 118ms/epoch - 4ms/step\n",
      "Epoch 50/200\n",
      "33/33 - 0s - loss: 0.1089 - accuracy: 0.9592 - 118ms/epoch - 4ms/step\n",
      "Epoch 51/200\n",
      "33/33 - 0s - loss: 0.1075 - accuracy: 0.9599 - 118ms/epoch - 4ms/step\n",
      "Epoch 52/200\n",
      "33/33 - 0s - loss: 0.1073 - accuracy: 0.9598 - 116ms/epoch - 4ms/step\n",
      "Epoch 53/200\n",
      "33/33 - 0s - loss: 0.1071 - accuracy: 0.9597 - 116ms/epoch - 4ms/step\n",
      "Epoch 54/200\n",
      "33/33 - 0s - loss: 0.1063 - accuracy: 0.9609 - 115ms/epoch - 3ms/step\n",
      "Epoch 55/200\n",
      "33/33 - 0s - loss: 0.1056 - accuracy: 0.9611 - 116ms/epoch - 4ms/step\n",
      "Epoch 56/200\n",
      "33/33 - 0s - loss: 0.1051 - accuracy: 0.9611 - 117ms/epoch - 4ms/step\n",
      "Epoch 57/200\n",
      "33/33 - 0s - loss: 0.1047 - accuracy: 0.9616 - 119ms/epoch - 4ms/step\n",
      "Epoch 58/200\n",
      "33/33 - 0s - loss: 0.1047 - accuracy: 0.9616 - 118ms/epoch - 4ms/step\n",
      "Epoch 59/200\n",
      "33/33 - 0s - loss: 0.1042 - accuracy: 0.9616 - 121ms/epoch - 4ms/step\n",
      "Epoch 60/200\n",
      "33/33 - 0s - loss: 0.1039 - accuracy: 0.9611 - 116ms/epoch - 4ms/step\n",
      "Epoch 61/200\n",
      "33/33 - 0s - loss: 0.1037 - accuracy: 0.9611 - 116ms/epoch - 4ms/step\n",
      "Epoch 62/200\n",
      "33/33 - 0s - loss: 0.1032 - accuracy: 0.9614 - 117ms/epoch - 4ms/step\n",
      "Epoch 63/200\n",
      "33/33 - 0s - loss: 0.1028 - accuracy: 0.9618 - 119ms/epoch - 4ms/step\n",
      "Epoch 64/200\n",
      "33/33 - 0s - loss: 0.1025 - accuracy: 0.9620 - 120ms/epoch - 4ms/step\n",
      "Epoch 65/200\n",
      "33/33 - 0s - loss: 0.1023 - accuracy: 0.9619 - 114ms/epoch - 3ms/step\n",
      "Epoch 66/200\n",
      "33/33 - 0s - loss: 0.1019 - accuracy: 0.9623 - 115ms/epoch - 3ms/step\n",
      "Epoch 67/200\n",
      "33/33 - 0s - loss: 0.1017 - accuracy: 0.9620 - 114ms/epoch - 3ms/step\n",
      "Epoch 68/200\n",
      "33/33 - 0s - loss: 0.1014 - accuracy: 0.9620 - 114ms/epoch - 3ms/step\n",
      "Epoch 69/200\n",
      "33/33 - 0s - loss: 0.1005 - accuracy: 0.9625 - 120ms/epoch - 4ms/step\n",
      "Epoch 70/200\n",
      "33/33 - 0s - loss: 0.1002 - accuracy: 0.9628 - 115ms/epoch - 3ms/step\n",
      "Epoch 71/200\n",
      "33/33 - 0s - loss: 0.0999 - accuracy: 0.9630 - 115ms/epoch - 3ms/step\n",
      "Epoch 72/200\n",
      "33/33 - 0s - loss: 0.0999 - accuracy: 0.9631 - 114ms/epoch - 3ms/step\n",
      "Epoch 73/200\n",
      "33/33 - 0s - loss: 0.0996 - accuracy: 0.9625 - 114ms/epoch - 3ms/step\n",
      "Epoch 74/200\n",
      "33/33 - 0s - loss: 0.0997 - accuracy: 0.9627 - 114ms/epoch - 3ms/step\n",
      "Epoch 75/200\n",
      "33/33 - 0s - loss: 0.0989 - accuracy: 0.9630 - 114ms/epoch - 3ms/step\n",
      "Epoch 76/200\n",
      "33/33 - 0s - loss: 0.0991 - accuracy: 0.9628 - 115ms/epoch - 3ms/step\n",
      "Epoch 77/200\n",
      "33/33 - 0s - loss: 0.0984 - accuracy: 0.9631 - 115ms/epoch - 3ms/step\n",
      "Epoch 78/200\n",
      "33/33 - 0s - loss: 0.0982 - accuracy: 0.9631 - 131ms/epoch - 4ms/step\n",
      "Epoch 79/200\n",
      "33/33 - 0s - loss: 0.0979 - accuracy: 0.9630 - 116ms/epoch - 4ms/step\n",
      "Epoch 80/200\n",
      "33/33 - 0s - loss: 0.0977 - accuracy: 0.9630 - 115ms/epoch - 3ms/step\n",
      "Epoch 81/200\n",
      "33/33 - 0s - loss: 0.0976 - accuracy: 0.9632 - 115ms/epoch - 3ms/step\n",
      "Epoch 82/200\n",
      "33/33 - 0s - loss: 0.0970 - accuracy: 0.9633 - 124ms/epoch - 4ms/step\n",
      "Epoch 83/200\n",
      "33/33 - 0s - loss: 0.0967 - accuracy: 0.9638 - 115ms/epoch - 3ms/step\n",
      "Epoch 84/200\n",
      "33/33 - 0s - loss: 0.0970 - accuracy: 0.9631 - 114ms/epoch - 3ms/step\n",
      "Epoch 85/200\n",
      "33/33 - 0s - loss: 0.0962 - accuracy: 0.9637 - 115ms/epoch - 3ms/step\n",
      "Epoch 86/200\n",
      "33/33 - 0s - loss: 0.0958 - accuracy: 0.9635 - 116ms/epoch - 4ms/step\n",
      "Epoch 87/200\n",
      "33/33 - 0s - loss: 0.0959 - accuracy: 0.9639 - 115ms/epoch - 3ms/step\n",
      "Epoch 88/200\n",
      "33/33 - 0s - loss: 0.0961 - accuracy: 0.9630 - 115ms/epoch - 3ms/step\n",
      "Epoch 89/200\n",
      "33/33 - 0s - loss: 0.0947 - accuracy: 0.9640 - 117ms/epoch - 4ms/step\n",
      "Epoch 90/200\n",
      "33/33 - 0s - loss: 0.0946 - accuracy: 0.9644 - 116ms/epoch - 4ms/step\n",
      "Epoch 91/200\n",
      "33/33 - 0s - loss: 0.0946 - accuracy: 0.9642 - 114ms/epoch - 3ms/step\n",
      "Epoch 92/200\n",
      "33/33 - 0s - loss: 0.0941 - accuracy: 0.9647 - 115ms/epoch - 3ms/step\n",
      "Epoch 93/200\n",
      "33/33 - 0s - loss: 0.0936 - accuracy: 0.9644 - 115ms/epoch - 3ms/step\n",
      "Epoch 94/200\n",
      "33/33 - 0s - loss: 0.0939 - accuracy: 0.9645 - 114ms/epoch - 3ms/step\n",
      "Epoch 95/200\n",
      "33/33 - 0s - loss: 0.0930 - accuracy: 0.9646 - 116ms/epoch - 4ms/step\n",
      "Epoch 96/200\n",
      "33/33 - 0s - loss: 0.0933 - accuracy: 0.9642 - 115ms/epoch - 3ms/step\n",
      "Epoch 97/200\n",
      "33/33 - 0s - loss: 0.0929 - accuracy: 0.9646 - 116ms/epoch - 4ms/step\n",
      "Epoch 98/200\n",
      "33/33 - 0s - loss: 0.0925 - accuracy: 0.9643 - 115ms/epoch - 3ms/step\n",
      "Epoch 99/200\n",
      "33/33 - 0s - loss: 0.0925 - accuracy: 0.9647 - 116ms/epoch - 4ms/step\n",
      "Epoch 100/200\n",
      "33/33 - 0s - loss: 0.0934 - accuracy: 0.9640 - 120ms/epoch - 4ms/step\n",
      "Epoch 101/200\n",
      "33/33 - 0s - loss: 0.0922 - accuracy: 0.9647 - 117ms/epoch - 4ms/step\n",
      "Epoch 102/200\n",
      "33/33 - 0s - loss: 0.0915 - accuracy: 0.9653 - 114ms/epoch - 3ms/step\n",
      "Epoch 103/200\n",
      "33/33 - 0s - loss: 0.0913 - accuracy: 0.9652 - 114ms/epoch - 3ms/step\n",
      "Epoch 104/200\n",
      "33/33 - 0s - loss: 0.0912 - accuracy: 0.9652 - 115ms/epoch - 3ms/step\n",
      "Epoch 105/200\n",
      "33/33 - 0s - loss: 0.0909 - accuracy: 0.9651 - 114ms/epoch - 3ms/step\n",
      "Epoch 106/200\n",
      "33/33 - 0s - loss: 0.0914 - accuracy: 0.9649 - 115ms/epoch - 3ms/step\n",
      "Epoch 107/200\n",
      "33/33 - 0s - loss: 0.0924 - accuracy: 0.9644 - 115ms/epoch - 3ms/step\n",
      "Epoch 108/200\n",
      "33/33 - 0s - loss: 0.0908 - accuracy: 0.9652 - 115ms/epoch - 3ms/step\n",
      "Epoch 109/200\n",
      "33/33 - 0s - loss: 0.0907 - accuracy: 0.9653 - 115ms/epoch - 3ms/step\n",
      "Epoch 110/200\n",
      "33/33 - 0s - loss: 0.0903 - accuracy: 0.9654 - 115ms/epoch - 3ms/step\n",
      "Epoch 111/200\n",
      "33/33 - 0s - loss: 0.0904 - accuracy: 0.9653 - 115ms/epoch - 3ms/step\n",
      "Epoch 112/200\n",
      "33/33 - 0s - loss: 0.0896 - accuracy: 0.9659 - 115ms/epoch - 3ms/step\n",
      "Epoch 113/200\n",
      "33/33 - 0s - loss: 0.0897 - accuracy: 0.9654 - 115ms/epoch - 3ms/step\n",
      "Epoch 114/200\n",
      "33/33 - 0s - loss: 0.0904 - accuracy: 0.9654 - 116ms/epoch - 4ms/step\n",
      "Epoch 115/200\n",
      "33/33 - 0s - loss: 0.0902 - accuracy: 0.9652 - 115ms/epoch - 3ms/step\n",
      "Epoch 116/200\n",
      "33/33 - 0s - loss: 0.0902 - accuracy: 0.9652 - 130ms/epoch - 4ms/step\n",
      "Epoch 117/200\n",
      "33/33 - 0s - loss: 0.0898 - accuracy: 0.9651 - 114ms/epoch - 3ms/step\n",
      "Epoch 118/200\n",
      "33/33 - 0s - loss: 0.0920 - accuracy: 0.9638 - 116ms/epoch - 4ms/step\n",
      "Epoch 119/200\n",
      "33/33 - 0s - loss: 0.0908 - accuracy: 0.9642 - 119ms/epoch - 4ms/step\n",
      "Epoch 120/200\n",
      "33/33 - 0s - loss: 0.0892 - accuracy: 0.9656 - 113ms/epoch - 3ms/step\n",
      "Epoch 121/200\n",
      "33/33 - 0s - loss: 0.0889 - accuracy: 0.9658 - 113ms/epoch - 3ms/step\n",
      "Epoch 122/200\n",
      "33/33 - 0s - loss: 0.0885 - accuracy: 0.9661 - 111ms/epoch - 3ms/step\n",
      "Epoch 123/200\n",
      "33/33 - 0s - loss: 0.0886 - accuracy: 0.9659 - 112ms/epoch - 3ms/step\n",
      "Epoch 124/200\n",
      "33/33 - 0s - loss: 0.0883 - accuracy: 0.9662 - 113ms/epoch - 3ms/step\n",
      "Epoch 125/200\n",
      "33/33 - 0s - loss: 0.0881 - accuracy: 0.9659 - 114ms/epoch - 3ms/step\n",
      "Epoch 126/200\n",
      "33/33 - 0s - loss: 0.0886 - accuracy: 0.9655 - 114ms/epoch - 3ms/step\n",
      "Epoch 127/200\n",
      "33/33 - 0s - loss: 0.0881 - accuracy: 0.9665 - 114ms/epoch - 3ms/step\n",
      "Epoch 128/200\n",
      "33/33 - 0s - loss: 0.0883 - accuracy: 0.9658 - 113ms/epoch - 3ms/step\n",
      "Epoch 129/200\n",
      "33/33 - 0s - loss: 0.0874 - accuracy: 0.9665 - 113ms/epoch - 3ms/step\n",
      "Epoch 130/200\n",
      "33/33 - 0s - loss: 0.0877 - accuracy: 0.9663 - 114ms/epoch - 3ms/step\n",
      "Epoch 131/200\n",
      "33/33 - 0s - loss: 0.0876 - accuracy: 0.9662 - 114ms/epoch - 3ms/step\n",
      "Epoch 132/200\n",
      "33/33 - 0s - loss: 0.0876 - accuracy: 0.9661 - 115ms/epoch - 3ms/step\n",
      "Epoch 133/200\n",
      "33/33 - 0s - loss: 0.0884 - accuracy: 0.9655 - 115ms/epoch - 3ms/step\n",
      "Epoch 134/200\n",
      "33/33 - 0s - loss: 0.0877 - accuracy: 0.9663 - 116ms/epoch - 4ms/step\n",
      "Epoch 135/200\n",
      "33/33 - 0s - loss: 0.0873 - accuracy: 0.9662 - 115ms/epoch - 3ms/step\n",
      "Epoch 136/200\n",
      "33/33 - 0s - loss: 0.0869 - accuracy: 0.9667 - 115ms/epoch - 3ms/step\n",
      "Epoch 137/200\n",
      "33/33 - 0s - loss: 0.0879 - accuracy: 0.9658 - 119ms/epoch - 4ms/step\n",
      "Epoch 138/200\n",
      "33/33 - 0s - loss: 0.0869 - accuracy: 0.9666 - 123ms/epoch - 4ms/step\n",
      "Epoch 139/200\n",
      "33/33 - 0s - loss: 0.0870 - accuracy: 0.9667 - 113ms/epoch - 3ms/step\n",
      "Epoch 140/200\n",
      "33/33 - 0s - loss: 0.0868 - accuracy: 0.9661 - 114ms/epoch - 3ms/step\n",
      "Epoch 141/200\n",
      "33/33 - 0s - loss: 0.0866 - accuracy: 0.9667 - 115ms/epoch - 3ms/step\n",
      "Epoch 142/200\n",
      "33/33 - 0s - loss: 0.0868 - accuracy: 0.9665 - 113ms/epoch - 3ms/step\n",
      "Epoch 143/200\n",
      "33/33 - 0s - loss: 0.0868 - accuracy: 0.9661 - 115ms/epoch - 3ms/step\n",
      "Epoch 144/200\n",
      "33/33 - 0s - loss: 0.0864 - accuracy: 0.9664 - 116ms/epoch - 4ms/step\n",
      "Epoch 145/200\n",
      "33/33 - 0s - loss: 0.0872 - accuracy: 0.9661 - 121ms/epoch - 4ms/step\n",
      "Epoch 146/200\n",
      "33/33 - 0s - loss: 0.0864 - accuracy: 0.9659 - 116ms/epoch - 4ms/step\n",
      "Epoch 147/200\n",
      "33/33 - 0s - loss: 0.0864 - accuracy: 0.9655 - 115ms/epoch - 3ms/step\n",
      "Epoch 148/200\n",
      "33/33 - 0s - loss: 0.0860 - accuracy: 0.9666 - 116ms/epoch - 4ms/step\n",
      "Epoch 149/200\n",
      "33/33 - 0s - loss: 0.0863 - accuracy: 0.9665 - 114ms/epoch - 3ms/step\n",
      "Epoch 150/200\n",
      "33/33 - 0s - loss: 0.0862 - accuracy: 0.9664 - 116ms/epoch - 4ms/step\n",
      "Epoch 151/200\n",
      "33/33 - 0s - loss: 0.0856 - accuracy: 0.9668 - 115ms/epoch - 3ms/step\n",
      "Epoch 152/200\n",
      "33/33 - 0s - loss: 0.0854 - accuracy: 0.9665 - 114ms/epoch - 3ms/step\n",
      "Epoch 153/200\n",
      "33/33 - 0s - loss: 0.0852 - accuracy: 0.9672 - 138ms/epoch - 4ms/step\n",
      "Epoch 154/200\n",
      "33/33 - 0s - loss: 0.0854 - accuracy: 0.9666 - 125ms/epoch - 4ms/step\n",
      "Epoch 155/200\n",
      "33/33 - 0s - loss: 0.0852 - accuracy: 0.9670 - 128ms/epoch - 4ms/step\n",
      "Epoch 156/200\n",
      "33/33 - 0s - loss: 0.0855 - accuracy: 0.9668 - 123ms/epoch - 4ms/step\n",
      "Epoch 157/200\n",
      "33/33 - 0s - loss: 0.0849 - accuracy: 0.9673 - 123ms/epoch - 4ms/step\n",
      "Epoch 158/200\n",
      "33/33 - 0s - loss: 0.0854 - accuracy: 0.9663 - 122ms/epoch - 4ms/step\n",
      "Epoch 159/200\n",
      "33/33 - 0s - loss: 0.0850 - accuracy: 0.9666 - 120ms/epoch - 4ms/step\n",
      "Epoch 160/200\n",
      "33/33 - 0s - loss: 0.0849 - accuracy: 0.9669 - 122ms/epoch - 4ms/step\n",
      "Epoch 161/200\n",
      "33/33 - 0s - loss: 0.0847 - accuracy: 0.9671 - 115ms/epoch - 3ms/step\n",
      "Epoch 162/200\n",
      "33/33 - 0s - loss: 0.0845 - accuracy: 0.9673 - 117ms/epoch - 4ms/step\n",
      "Epoch 163/200\n",
      "33/33 - 0s - loss: 0.0846 - accuracy: 0.9670 - 115ms/epoch - 3ms/step\n",
      "Epoch 164/200\n",
      "33/33 - 0s - loss: 0.0854 - accuracy: 0.9662 - 116ms/epoch - 4ms/step\n",
      "Epoch 165/200\n",
      "33/33 - 0s - loss: 0.0841 - accuracy: 0.9677 - 125ms/epoch - 4ms/step\n",
      "Epoch 166/200\n",
      "33/33 - 0s - loss: 0.0848 - accuracy: 0.9673 - 130ms/epoch - 4ms/step\n",
      "Epoch 167/200\n",
      "33/33 - 0s - loss: 0.0847 - accuracy: 0.9671 - 128ms/epoch - 4ms/step\n",
      "Epoch 168/200\n",
      "33/33 - 0s - loss: 0.0841 - accuracy: 0.9674 - 127ms/epoch - 4ms/step\n",
      "Epoch 169/200\n",
      "33/33 - 0s - loss: 0.0840 - accuracy: 0.9679 - 124ms/epoch - 4ms/step\n",
      "Epoch 170/200\n",
      "33/33 - 0s - loss: 0.0852 - accuracy: 0.9665 - 126ms/epoch - 4ms/step\n",
      "Epoch 171/200\n",
      "33/33 - 0s - loss: 0.0838 - accuracy: 0.9675 - 125ms/epoch - 4ms/step\n",
      "Epoch 172/200\n",
      "33/33 - 0s - loss: 0.0839 - accuracy: 0.9674 - 118ms/epoch - 4ms/step\n",
      "Epoch 173/200\n",
      "33/33 - 0s - loss: 0.0838 - accuracy: 0.9674 - 122ms/epoch - 4ms/step\n",
      "Epoch 174/200\n",
      "33/33 - 0s - loss: 0.0841 - accuracy: 0.9674 - 117ms/epoch - 4ms/step\n",
      "Epoch 175/200\n",
      "33/33 - 0s - loss: 0.0855 - accuracy: 0.9665 - 119ms/epoch - 4ms/step\n",
      "Epoch 176/200\n",
      "33/33 - 0s - loss: 0.0852 - accuracy: 0.9667 - 117ms/epoch - 4ms/step\n",
      "Epoch 177/200\n",
      "33/33 - 0s - loss: 0.0835 - accuracy: 0.9675 - 116ms/epoch - 4ms/step\n",
      "Epoch 178/200\n",
      "33/33 - 0s - loss: 0.0846 - accuracy: 0.9670 - 115ms/epoch - 3ms/step\n",
      "Epoch 179/200\n",
      "33/33 - 0s - loss: 0.0838 - accuracy: 0.9670 - 115ms/epoch - 3ms/step\n",
      "Epoch 180/200\n",
      "33/33 - 0s - loss: 0.0837 - accuracy: 0.9673 - 117ms/epoch - 4ms/step\n",
      "Epoch 181/200\n",
      "33/33 - 0s - loss: 0.0835 - accuracy: 0.9673 - 118ms/epoch - 4ms/step\n",
      "Epoch 182/200\n",
      "33/33 - 0s - loss: 0.0834 - accuracy: 0.9677 - 118ms/epoch - 4ms/step\n",
      "Epoch 183/200\n",
      "33/33 - 0s - loss: 0.0832 - accuracy: 0.9678 - 116ms/epoch - 4ms/step\n",
      "Epoch 184/200\n",
      "33/33 - 0s - loss: 0.0832 - accuracy: 0.9678 - 116ms/epoch - 4ms/step\n",
      "Epoch 185/200\n",
      "33/33 - 0s - loss: 0.0829 - accuracy: 0.9675 - 117ms/epoch - 4ms/step\n",
      "Epoch 186/200\n",
      "33/33 - 0s - loss: 0.0828 - accuracy: 0.9681 - 115ms/epoch - 3ms/step\n",
      "Epoch 187/200\n",
      "33/33 - 0s - loss: 0.0828 - accuracy: 0.9679 - 118ms/epoch - 4ms/step\n",
      "Epoch 188/200\n",
      "33/33 - 0s - loss: 0.0828 - accuracy: 0.9676 - 117ms/epoch - 4ms/step\n",
      "Epoch 189/200\n",
      "33/33 - 0s - loss: 0.0832 - accuracy: 0.9677 - 115ms/epoch - 3ms/step\n",
      "Epoch 190/200\n",
      "33/33 - 0s - loss: 0.0836 - accuracy: 0.9672 - 116ms/epoch - 4ms/step\n",
      "Epoch 191/200\n",
      "33/33 - 0s - loss: 0.0833 - accuracy: 0.9679 - 138ms/epoch - 4ms/step\n",
      "Epoch 192/200\n",
      "33/33 - 0s - loss: 0.0835 - accuracy: 0.9673 - 115ms/epoch - 3ms/step\n",
      "Epoch 193/200\n",
      "33/33 - 0s - loss: 0.0825 - accuracy: 0.9679 - 114ms/epoch - 3ms/step\n",
      "Epoch 194/200\n",
      "33/33 - 0s - loss: 0.0837 - accuracy: 0.9671 - 114ms/epoch - 3ms/step\n",
      "Epoch 195/200\n",
      "33/33 - 0s - loss: 0.0824 - accuracy: 0.9678 - 115ms/epoch - 3ms/step\n",
      "Epoch 196/200\n",
      "33/33 - 0s - loss: 0.0823 - accuracy: 0.9683 - 116ms/epoch - 4ms/step\n",
      "Epoch 197/200\n",
      "33/33 - 0s - loss: 0.0826 - accuracy: 0.9680 - 115ms/epoch - 3ms/step\n",
      "Epoch 198/200\n",
      "33/33 - 0s - loss: 0.0825 - accuracy: 0.9680 - 115ms/epoch - 3ms/step\n",
      "Epoch 199/200\n",
      "33/33 - 0s - loss: 0.0821 - accuracy: 0.9681 - 114ms/epoch - 3ms/step\n",
      "Epoch 200/200\n",
      "33/33 - 0s - loss: 0.0826 - accuracy: 0.9675 - 116ms/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "#Build the neural network model\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(GRU(20, return_sequences=True,input_shape=(1,56)))\n",
    "    model.add(GRU(20, return_sequences=True))\n",
    "    model.add(Dense(10, activation='softmax')) #for multiclass classification\n",
    "    #Compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',\n",
    "                  # metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "                  metrics=['accuracy']\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "#The GRU input layer must be 3D.\n",
    "#The meaning of the 3 input dimensions are: samples, time steps, and features.\n",
    "#reshape input data\n",
    "X_train_array = array(X_train) #array has been declared in the previous cell\n",
    "print(len(X_train_array))\n",
    "X_train_reshaped = X_train_array.reshape(X_train_array.shape[0],1,56)\n",
    "\n",
    "#reshape output data\n",
    "X_test_array=  array(X_test)\n",
    "X_test_reshaped = X_test_array.reshape(X_test_array.shape[0],1,56) \n",
    "\n",
    "\n",
    "#institate the model\n",
    "model = build_model()\n",
    "\n",
    "start = time.time()\n",
    "#fit the model\n",
    "model.fit(X_train_reshaped, y_train, epochs=200, batch_size=2000,verbose=2)\n",
    "end_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 1s 944us/step - loss: 0.0862 - accuracy: 0.9661\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "# loss, accuracy, f1s, precision, recall = model.evaluate(X_test_reshaped, y_test)\n",
    "end_predict = time.time()\n",
    "model_performance.loc['GRU (Keras)'] = [accuracy, accuracy, accuracy, accuracy, end_train-start,end_predict-end_train,end_predict-start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82332, 56)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65865\n",
      "Epoch 1/200\n",
      "33/33 - 2s - loss: 2.2007 - accuracy: 0.6398 - 2s/epoch - 66ms/step\n",
      "Epoch 2/200\n",
      "33/33 - 0s - loss: 1.7996 - accuracy: 0.7107 - 129ms/epoch - 4ms/step\n",
      "Epoch 3/200\n",
      "33/33 - 0s - loss: 1.1588 - accuracy: 0.7062 - 129ms/epoch - 4ms/step\n",
      "Epoch 4/200\n",
      "33/33 - 0s - loss: 0.7620 - accuracy: 0.7309 - 132ms/epoch - 4ms/step\n",
      "Epoch 5/200\n",
      "33/33 - 0s - loss: 0.6118 - accuracy: 0.7485 - 130ms/epoch - 4ms/step\n",
      "Epoch 6/200\n",
      "33/33 - 0s - loss: 0.5353 - accuracy: 0.7486 - 126ms/epoch - 4ms/step\n",
      "Epoch 7/200\n",
      "33/33 - 0s - loss: 0.4725 - accuracy: 0.7683 - 132ms/epoch - 4ms/step\n",
      "Epoch 8/200\n",
      "33/33 - 0s - loss: 0.4104 - accuracy: 0.8027 - 132ms/epoch - 4ms/step\n",
      "Epoch 9/200\n",
      "33/33 - 0s - loss: 0.3510 - accuracy: 0.8542 - 145ms/epoch - 4ms/step\n",
      "Epoch 10/200\n",
      "33/33 - 0s - loss: 0.3042 - accuracy: 0.8807 - 152ms/epoch - 5ms/step\n",
      "Epoch 11/200\n",
      "33/33 - 0s - loss: 0.2692 - accuracy: 0.8928 - 140ms/epoch - 4ms/step\n",
      "Epoch 12/200\n",
      "33/33 - 0s - loss: 0.2444 - accuracy: 0.8995 - 135ms/epoch - 4ms/step\n",
      "Epoch 13/200\n",
      "33/33 - 0s - loss: 0.2276 - accuracy: 0.9046 - 134ms/epoch - 4ms/step\n",
      "Epoch 14/200\n",
      "33/33 - 0s - loss: 0.2153 - accuracy: 0.9085 - 136ms/epoch - 4ms/step\n",
      "Epoch 15/200\n",
      "33/33 - 0s - loss: 0.2058 - accuracy: 0.9115 - 127ms/epoch - 4ms/step\n",
      "Epoch 16/200\n",
      "33/33 - 0s - loss: 0.1981 - accuracy: 0.9132 - 124ms/epoch - 4ms/step\n",
      "Epoch 17/200\n",
      "33/33 - 0s - loss: 0.1920 - accuracy: 0.9149 - 139ms/epoch - 4ms/step\n",
      "Epoch 18/200\n",
      "33/33 - 0s - loss: 0.1862 - accuracy: 0.9177 - 124ms/epoch - 4ms/step\n",
      "Epoch 19/200\n",
      "33/33 - 0s - loss: 0.1804 - accuracy: 0.9210 - 123ms/epoch - 4ms/step\n",
      "Epoch 20/200\n",
      "33/33 - 0s - loss: 0.1743 - accuracy: 0.9251 - 123ms/epoch - 4ms/step\n",
      "Epoch 21/200\n",
      "33/33 - 0s - loss: 0.1676 - accuracy: 0.9303 - 124ms/epoch - 4ms/step\n",
      "Epoch 22/200\n",
      "33/33 - 0s - loss: 0.1609 - accuracy: 0.9349 - 124ms/epoch - 4ms/step\n",
      "Epoch 23/200\n",
      "33/33 - 0s - loss: 0.1550 - accuracy: 0.9385 - 127ms/epoch - 4ms/step\n",
      "Epoch 24/200\n",
      "33/33 - 0s - loss: 0.1500 - accuracy: 0.9420 - 126ms/epoch - 4ms/step\n",
      "Epoch 25/200\n",
      "33/33 - 0s - loss: 0.1465 - accuracy: 0.9432 - 126ms/epoch - 4ms/step\n",
      "Epoch 26/200\n",
      "33/33 - 0s - loss: 0.1426 - accuracy: 0.9456 - 131ms/epoch - 4ms/step\n",
      "Epoch 27/200\n",
      "33/33 - 0s - loss: 0.1395 - accuracy: 0.9475 - 126ms/epoch - 4ms/step\n",
      "Epoch 28/200\n",
      "33/33 - 0s - loss: 0.1367 - accuracy: 0.9489 - 122ms/epoch - 4ms/step\n",
      "Epoch 29/200\n",
      "33/33 - 0s - loss: 0.1341 - accuracy: 0.9503 - 122ms/epoch - 4ms/step\n",
      "Epoch 30/200\n",
      "33/33 - 0s - loss: 0.1326 - accuracy: 0.9497 - 123ms/epoch - 4ms/step\n",
      "Epoch 31/200\n",
      "33/33 - 0s - loss: 0.1302 - accuracy: 0.9517 - 123ms/epoch - 4ms/step\n",
      "Epoch 32/200\n",
      "33/33 - 0s - loss: 0.1285 - accuracy: 0.9515 - 124ms/epoch - 4ms/step\n",
      "Epoch 33/200\n",
      "33/33 - 0s - loss: 0.1265 - accuracy: 0.9533 - 122ms/epoch - 4ms/step\n",
      "Epoch 34/200\n",
      "33/33 - 0s - loss: 0.1246 - accuracy: 0.9541 - 125ms/epoch - 4ms/step\n",
      "Epoch 35/200\n",
      "33/33 - 0s - loss: 0.1231 - accuracy: 0.9550 - 123ms/epoch - 4ms/step\n",
      "Epoch 36/200\n",
      "33/33 - 0s - loss: 0.1221 - accuracy: 0.9550 - 123ms/epoch - 4ms/step\n",
      "Epoch 37/200\n",
      "33/33 - 0s - loss: 0.1205 - accuracy: 0.9563 - 122ms/epoch - 4ms/step\n",
      "Epoch 38/200\n",
      "33/33 - 0s - loss: 0.1198 - accuracy: 0.9557 - 123ms/epoch - 4ms/step\n",
      "Epoch 39/200\n",
      "33/33 - 0s - loss: 0.1182 - accuracy: 0.9565 - 124ms/epoch - 4ms/step\n",
      "Epoch 40/200\n",
      "33/33 - 0s - loss: 0.1171 - accuracy: 0.9569 - 151ms/epoch - 5ms/step\n",
      "Epoch 41/200\n",
      "33/33 - 0s - loss: 0.1164 - accuracy: 0.9574 - 151ms/epoch - 5ms/step\n",
      "Epoch 42/200\n",
      "33/33 - 0s - loss: 0.1157 - accuracy: 0.9571 - 147ms/epoch - 4ms/step\n",
      "Epoch 43/200\n",
      "33/33 - 0s - loss: 0.1145 - accuracy: 0.9582 - 150ms/epoch - 5ms/step\n",
      "Epoch 44/200\n",
      "33/33 - 0s - loss: 0.1140 - accuracy: 0.9582 - 149ms/epoch - 5ms/step\n",
      "Epoch 45/200\n",
      "33/33 - 0s - loss: 0.1133 - accuracy: 0.9582 - 151ms/epoch - 5ms/step\n",
      "Epoch 46/200\n",
      "33/33 - 0s - loss: 0.1126 - accuracy: 0.9587 - 153ms/epoch - 5ms/step\n",
      "Epoch 47/200\n",
      "33/33 - 0s - loss: 0.1118 - accuracy: 0.9591 - 146ms/epoch - 4ms/step\n",
      "Epoch 48/200\n",
      "33/33 - 0s - loss: 0.1113 - accuracy: 0.9591 - 153ms/epoch - 5ms/step\n",
      "Epoch 49/200\n",
      "33/33 - 0s - loss: 0.1104 - accuracy: 0.9594 - 155ms/epoch - 5ms/step\n",
      "Epoch 50/200\n",
      "33/33 - 0s - loss: 0.1103 - accuracy: 0.9591 - 154ms/epoch - 5ms/step\n",
      "Epoch 51/200\n",
      "33/33 - 0s - loss: 0.1097 - accuracy: 0.9600 - 156ms/epoch - 5ms/step\n",
      "Epoch 52/200\n",
      "33/33 - 0s - loss: 0.1090 - accuracy: 0.9602 - 164ms/epoch - 5ms/step\n",
      "Epoch 53/200\n",
      "33/33 - 0s - loss: 0.1087 - accuracy: 0.9603 - 158ms/epoch - 5ms/step\n",
      "Epoch 54/200\n",
      "33/33 - 0s - loss: 0.1082 - accuracy: 0.9601 - 154ms/epoch - 5ms/step\n",
      "Epoch 55/200\n",
      "33/33 - 0s - loss: 0.1080 - accuracy: 0.9606 - 152ms/epoch - 5ms/step\n",
      "Epoch 56/200\n",
      "33/33 - 0s - loss: 0.1081 - accuracy: 0.9599 - 169ms/epoch - 5ms/step\n",
      "Epoch 57/200\n",
      "33/33 - 0s - loss: 0.1067 - accuracy: 0.9613 - 160ms/epoch - 5ms/step\n",
      "Epoch 58/200\n",
      "33/33 - 0s - loss: 0.1068 - accuracy: 0.9604 - 155ms/epoch - 5ms/step\n",
      "Epoch 59/200\n",
      "33/33 - 0s - loss: 0.1061 - accuracy: 0.9613 - 153ms/epoch - 5ms/step\n",
      "Epoch 60/200\n",
      "33/33 - 0s - loss: 0.1060 - accuracy: 0.9605 - 156ms/epoch - 5ms/step\n",
      "Epoch 61/200\n",
      "33/33 - 0s - loss: 0.1057 - accuracy: 0.9612 - 154ms/epoch - 5ms/step\n",
      "Epoch 62/200\n",
      "33/33 - 0s - loss: 0.1050 - accuracy: 0.9613 - 156ms/epoch - 5ms/step\n",
      "Epoch 63/200\n",
      "33/33 - 0s - loss: 0.1042 - accuracy: 0.9616 - 153ms/epoch - 5ms/step\n",
      "Epoch 64/200\n",
      "33/33 - 0s - loss: 0.1039 - accuracy: 0.9617 - 153ms/epoch - 5ms/step\n",
      "Epoch 65/200\n",
      "33/33 - 0s - loss: 0.1037 - accuracy: 0.9622 - 155ms/epoch - 5ms/step\n",
      "Epoch 66/200\n",
      "33/33 - 0s - loss: 0.1041 - accuracy: 0.9609 - 152ms/epoch - 5ms/step\n",
      "Epoch 67/200\n",
      "33/33 - 0s - loss: 0.1029 - accuracy: 0.9627 - 154ms/epoch - 5ms/step\n",
      "Epoch 68/200\n",
      "33/33 - 0s - loss: 0.1027 - accuracy: 0.9620 - 153ms/epoch - 5ms/step\n",
      "Epoch 69/200\n",
      "33/33 - 0s - loss: 0.1023 - accuracy: 0.9626 - 158ms/epoch - 5ms/step\n",
      "Epoch 70/200\n",
      "33/33 - 0s - loss: 0.1023 - accuracy: 0.9617 - 157ms/epoch - 5ms/step\n",
      "Epoch 71/200\n",
      "33/33 - 0s - loss: 0.1016 - accuracy: 0.9624 - 156ms/epoch - 5ms/step\n",
      "Epoch 72/200\n",
      "33/33 - 0s - loss: 0.1012 - accuracy: 0.9631 - 152ms/epoch - 5ms/step\n",
      "Epoch 73/200\n",
      "33/33 - 0s - loss: 0.1010 - accuracy: 0.9627 - 153ms/epoch - 5ms/step\n",
      "Epoch 74/200\n",
      "33/33 - 0s - loss: 0.1011 - accuracy: 0.9621 - 155ms/epoch - 5ms/step\n",
      "Epoch 75/200\n",
      "33/33 - 0s - loss: 0.1003 - accuracy: 0.9627 - 155ms/epoch - 5ms/step\n",
      "Epoch 76/200\n",
      "33/33 - 0s - loss: 0.1001 - accuracy: 0.9630 - 156ms/epoch - 5ms/step\n",
      "Epoch 77/200\n",
      "33/33 - 0s - loss: 0.0998 - accuracy: 0.9631 - 154ms/epoch - 5ms/step\n",
      "Epoch 78/200\n",
      "33/33 - 0s - loss: 0.0997 - accuracy: 0.9624 - 154ms/epoch - 5ms/step\n",
      "Epoch 79/200\n",
      "33/33 - 0s - loss: 0.0994 - accuracy: 0.9627 - 153ms/epoch - 5ms/step\n",
      "Epoch 80/200\n",
      "33/33 - 0s - loss: 0.0992 - accuracy: 0.9635 - 153ms/epoch - 5ms/step\n",
      "Epoch 81/200\n",
      "33/33 - 0s - loss: 0.0987 - accuracy: 0.9634 - 152ms/epoch - 5ms/step\n",
      "Epoch 82/200\n",
      "33/33 - 0s - loss: 0.0987 - accuracy: 0.9632 - 154ms/epoch - 5ms/step\n",
      "Epoch 83/200\n",
      "33/33 - 0s - loss: 0.0981 - accuracy: 0.9632 - 154ms/epoch - 5ms/step\n",
      "Epoch 84/200\n",
      "33/33 - 0s - loss: 0.0977 - accuracy: 0.9637 - 160ms/epoch - 5ms/step\n",
      "Epoch 85/200\n",
      "33/33 - 0s - loss: 0.0979 - accuracy: 0.9631 - 155ms/epoch - 5ms/step\n",
      "Epoch 86/200\n",
      "33/33 - 0s - loss: 0.0982 - accuracy: 0.9629 - 153ms/epoch - 5ms/step\n",
      "Epoch 87/200\n",
      "33/33 - 0s - loss: 0.0972 - accuracy: 0.9638 - 151ms/epoch - 5ms/step\n",
      "Epoch 88/200\n",
      "33/33 - 0s - loss: 0.0970 - accuracy: 0.9635 - 152ms/epoch - 5ms/step\n",
      "Epoch 89/200\n",
      "33/33 - 0s - loss: 0.0971 - accuracy: 0.9640 - 153ms/epoch - 5ms/step\n",
      "Epoch 90/200\n",
      "33/33 - 0s - loss: 0.0972 - accuracy: 0.9633 - 151ms/epoch - 5ms/step\n",
      "Epoch 91/200\n",
      "33/33 - 0s - loss: 0.0967 - accuracy: 0.9637 - 141ms/epoch - 4ms/step\n",
      "Epoch 92/200\n",
      "33/33 - 0s - loss: 0.0963 - accuracy: 0.9638 - 139ms/epoch - 4ms/step\n",
      "Epoch 93/200\n",
      "33/33 - 0s - loss: 0.0957 - accuracy: 0.9642 - 148ms/epoch - 4ms/step\n",
      "Epoch 94/200\n",
      "33/33 - 0s - loss: 0.0962 - accuracy: 0.9641 - 152ms/epoch - 5ms/step\n",
      "Epoch 95/200\n",
      "33/33 - 0s - loss: 0.0954 - accuracy: 0.9643 - 139ms/epoch - 4ms/step\n",
      "Epoch 96/200\n",
      "33/33 - 0s - loss: 0.0953 - accuracy: 0.9643 - 138ms/epoch - 4ms/step\n",
      "Epoch 97/200\n",
      "33/33 - 0s - loss: 0.0953 - accuracy: 0.9641 - 155ms/epoch - 5ms/step\n",
      "Epoch 98/200\n",
      "33/33 - 0s - loss: 0.0950 - accuracy: 0.9641 - 145ms/epoch - 4ms/step\n",
      "Epoch 99/200\n",
      "33/33 - 0s - loss: 0.0954 - accuracy: 0.9647 - 148ms/epoch - 4ms/step\n",
      "Epoch 100/200\n",
      "33/33 - 0s - loss: 0.0946 - accuracy: 0.9648 - 141ms/epoch - 4ms/step\n",
      "Epoch 101/200\n",
      "33/33 - 0s - loss: 0.0944 - accuracy: 0.9650 - 140ms/epoch - 4ms/step\n",
      "Epoch 102/200\n",
      "33/33 - 0s - loss: 0.0940 - accuracy: 0.9647 - 139ms/epoch - 4ms/step\n",
      "Epoch 103/200\n",
      "33/33 - 0s - loss: 0.0941 - accuracy: 0.9651 - 138ms/epoch - 4ms/step\n",
      "Epoch 104/200\n",
      "33/33 - 0s - loss: 0.0945 - accuracy: 0.9644 - 147ms/epoch - 4ms/step\n",
      "Epoch 105/200\n",
      "33/33 - 0s - loss: 0.0937 - accuracy: 0.9648 - 146ms/epoch - 4ms/step\n",
      "Epoch 106/200\n",
      "33/33 - 0s - loss: 0.0935 - accuracy: 0.9652 - 156ms/epoch - 5ms/step\n",
      "Epoch 107/200\n",
      "33/33 - 0s - loss: 0.0938 - accuracy: 0.9649 - 144ms/epoch - 4ms/step\n",
      "Epoch 108/200\n",
      "33/33 - 0s - loss: 0.0936 - accuracy: 0.9649 - 152ms/epoch - 5ms/step\n",
      "Epoch 109/200\n",
      "33/33 - 0s - loss: 0.0933 - accuracy: 0.9653 - 149ms/epoch - 5ms/step\n",
      "Epoch 110/200\n",
      "33/33 - 0s - loss: 0.0931 - accuracy: 0.9655 - 147ms/epoch - 4ms/step\n",
      "Epoch 111/200\n",
      "33/33 - 0s - loss: 0.0933 - accuracy: 0.9650 - 147ms/epoch - 4ms/step\n",
      "Epoch 112/200\n",
      "33/33 - 0s - loss: 0.0930 - accuracy: 0.9652 - 149ms/epoch - 5ms/step\n",
      "Epoch 113/200\n",
      "33/33 - 0s - loss: 0.0926 - accuracy: 0.9657 - 151ms/epoch - 5ms/step\n",
      "Epoch 114/200\n",
      "33/33 - 0s - loss: 0.0927 - accuracy: 0.9660 - 144ms/epoch - 4ms/step\n",
      "Epoch 115/200\n",
      "33/33 - 0s - loss: 0.0927 - accuracy: 0.9657 - 138ms/epoch - 4ms/step\n",
      "Epoch 116/200\n",
      "33/33 - 0s - loss: 0.0929 - accuracy: 0.9649 - 139ms/epoch - 4ms/step\n",
      "Epoch 117/200\n",
      "33/33 - 0s - loss: 0.0925 - accuracy: 0.9650 - 141ms/epoch - 4ms/step\n",
      "Epoch 118/200\n",
      "33/33 - 0s - loss: 0.0921 - accuracy: 0.9653 - 140ms/epoch - 4ms/step\n",
      "Epoch 119/200\n",
      "33/33 - 0s - loss: 0.0920 - accuracy: 0.9652 - 156ms/epoch - 5ms/step\n",
      "Epoch 120/200\n",
      "33/33 - 0s - loss: 0.0919 - accuracy: 0.9659 - 182ms/epoch - 6ms/step\n",
      "Epoch 121/200\n",
      "33/33 - 0s - loss: 0.0917 - accuracy: 0.9658 - 142ms/epoch - 4ms/step\n",
      "Epoch 122/200\n",
      "33/33 - 0s - loss: 0.0916 - accuracy: 0.9655 - 135ms/epoch - 4ms/step\n",
      "Epoch 123/200\n",
      "33/33 - 0s - loss: 0.0915 - accuracy: 0.9659 - 137ms/epoch - 4ms/step\n",
      "Epoch 124/200\n",
      "33/33 - 0s - loss: 0.0915 - accuracy: 0.9658 - 132ms/epoch - 4ms/step\n",
      "Epoch 125/200\n",
      "33/33 - 0s - loss: 0.0917 - accuracy: 0.9656 - 132ms/epoch - 4ms/step\n",
      "Epoch 126/200\n",
      "33/33 - 0s - loss: 0.0912 - accuracy: 0.9661 - 137ms/epoch - 4ms/step\n",
      "Epoch 127/200\n",
      "33/33 - 0s - loss: 0.0910 - accuracy: 0.9661 - 123ms/epoch - 4ms/step\n",
      "Epoch 128/200\n",
      "33/33 - 0s - loss: 0.0913 - accuracy: 0.9659 - 129ms/epoch - 4ms/step\n",
      "Epoch 129/200\n",
      "33/33 - 0s - loss: 0.0909 - accuracy: 0.9659 - 129ms/epoch - 4ms/step\n",
      "Epoch 130/200\n",
      "33/33 - 0s - loss: 0.0908 - accuracy: 0.9661 - 122ms/epoch - 4ms/step\n",
      "Epoch 131/200\n",
      "33/33 - 0s - loss: 0.0906 - accuracy: 0.9659 - 126ms/epoch - 4ms/step\n",
      "Epoch 132/200\n",
      "33/33 - 0s - loss: 0.0905 - accuracy: 0.9664 - 124ms/epoch - 4ms/step\n",
      "Epoch 133/200\n",
      "33/33 - 0s - loss: 0.0904 - accuracy: 0.9661 - 131ms/epoch - 4ms/step\n",
      "Epoch 134/200\n",
      "33/33 - 0s - loss: 0.0904 - accuracy: 0.9663 - 131ms/epoch - 4ms/step\n",
      "Epoch 135/200\n",
      "33/33 - 0s - loss: 0.0904 - accuracy: 0.9666 - 125ms/epoch - 4ms/step\n",
      "Epoch 136/200\n",
      "33/33 - 0s - loss: 0.0901 - accuracy: 0.9662 - 126ms/epoch - 4ms/step\n",
      "Epoch 137/200\n",
      "33/33 - 0s - loss: 0.0907 - accuracy: 0.9659 - 124ms/epoch - 4ms/step\n",
      "Epoch 138/200\n",
      "33/33 - 0s - loss: 0.0906 - accuracy: 0.9656 - 126ms/epoch - 4ms/step\n",
      "Epoch 139/200\n",
      "33/33 - 0s - loss: 0.0905 - accuracy: 0.9662 - 124ms/epoch - 4ms/step\n",
      "Epoch 140/200\n",
      "33/33 - 0s - loss: 0.0900 - accuracy: 0.9664 - 123ms/epoch - 4ms/step\n",
      "Epoch 141/200\n",
      "33/33 - 0s - loss: 0.0897 - accuracy: 0.9664 - 122ms/epoch - 4ms/step\n",
      "Epoch 142/200\n",
      "33/33 - 0s - loss: 0.0898 - accuracy: 0.9663 - 123ms/epoch - 4ms/step\n",
      "Epoch 143/200\n",
      "33/33 - 0s - loss: 0.0895 - accuracy: 0.9669 - 136ms/epoch - 4ms/step\n",
      "Epoch 144/200\n",
      "33/33 - 0s - loss: 0.0892 - accuracy: 0.9668 - 129ms/epoch - 4ms/step\n",
      "Epoch 145/200\n",
      "33/33 - 0s - loss: 0.0892 - accuracy: 0.9665 - 141ms/epoch - 4ms/step\n",
      "Epoch 146/200\n",
      "33/33 - 0s - loss: 0.0897 - accuracy: 0.9662 - 126ms/epoch - 4ms/step\n",
      "Epoch 147/200\n",
      "33/33 - 0s - loss: 0.0894 - accuracy: 0.9664 - 142ms/epoch - 4ms/step\n",
      "Epoch 148/200\n",
      "33/33 - 0s - loss: 0.0894 - accuracy: 0.9662 - 131ms/epoch - 4ms/step\n",
      "Epoch 149/200\n",
      "33/33 - 0s - loss: 0.0892 - accuracy: 0.9666 - 127ms/epoch - 4ms/step\n",
      "Epoch 150/200\n",
      "33/33 - 0s - loss: 0.0889 - accuracy: 0.9666 - 129ms/epoch - 4ms/step\n",
      "Epoch 151/200\n",
      "33/33 - 0s - loss: 0.0890 - accuracy: 0.9667 - 138ms/epoch - 4ms/step\n",
      "Epoch 152/200\n",
      "33/33 - 0s - loss: 0.0890 - accuracy: 0.9667 - 134ms/epoch - 4ms/step\n",
      "Epoch 153/200\n",
      "33/33 - 0s - loss: 0.0897 - accuracy: 0.9661 - 123ms/epoch - 4ms/step\n",
      "Epoch 154/200\n",
      "33/33 - 0s - loss: 0.0895 - accuracy: 0.9663 - 123ms/epoch - 4ms/step\n",
      "Epoch 155/200\n",
      "33/33 - 0s - loss: 0.0887 - accuracy: 0.9671 - 137ms/epoch - 4ms/step\n",
      "Epoch 156/200\n",
      "33/33 - 0s - loss: 0.0885 - accuracy: 0.9667 - 136ms/epoch - 4ms/step\n",
      "Epoch 157/200\n",
      "33/33 - 0s - loss: 0.0883 - accuracy: 0.9671 - 132ms/epoch - 4ms/step\n",
      "Epoch 158/200\n",
      "33/33 - 0s - loss: 0.0894 - accuracy: 0.9660 - 124ms/epoch - 4ms/step\n",
      "Epoch 159/200\n",
      "33/33 - 0s - loss: 0.0888 - accuracy: 0.9664 - 122ms/epoch - 4ms/step\n",
      "Epoch 160/200\n",
      "33/33 - 0s - loss: 0.0881 - accuracy: 0.9666 - 123ms/epoch - 4ms/step\n",
      "Epoch 161/200\n",
      "33/33 - 0s - loss: 0.0885 - accuracy: 0.9667 - 126ms/epoch - 4ms/step\n",
      "Epoch 162/200\n",
      "33/33 - 0s - loss: 0.0882 - accuracy: 0.9666 - 126ms/epoch - 4ms/step\n",
      "Epoch 163/200\n",
      "33/33 - 0s - loss: 0.0881 - accuracy: 0.9669 - 124ms/epoch - 4ms/step\n",
      "Epoch 164/200\n",
      "33/33 - 0s - loss: 0.0886 - accuracy: 0.9667 - 124ms/epoch - 4ms/step\n",
      "Epoch 165/200\n",
      "33/33 - 0s - loss: 0.0880 - accuracy: 0.9667 - 122ms/epoch - 4ms/step\n",
      "Epoch 166/200\n",
      "33/33 - 0s - loss: 0.0882 - accuracy: 0.9667 - 129ms/epoch - 4ms/step\n",
      "Epoch 167/200\n",
      "33/33 - 0s - loss: 0.0883 - accuracy: 0.9670 - 123ms/epoch - 4ms/step\n",
      "Epoch 168/200\n",
      "33/33 - 0s - loss: 0.0875 - accuracy: 0.9673 - 124ms/epoch - 4ms/step\n",
      "Epoch 169/200\n",
      "33/33 - 0s - loss: 0.0875 - accuracy: 0.9674 - 123ms/epoch - 4ms/step\n",
      "Epoch 170/200\n",
      "33/33 - 0s - loss: 0.0877 - accuracy: 0.9669 - 122ms/epoch - 4ms/step\n",
      "Epoch 171/200\n",
      "33/33 - 0s - loss: 0.0875 - accuracy: 0.9672 - 124ms/epoch - 4ms/step\n",
      "Epoch 172/200\n",
      "33/33 - 0s - loss: 0.0872 - accuracy: 0.9675 - 123ms/epoch - 4ms/step\n",
      "Epoch 173/200\n",
      "33/33 - 0s - loss: 0.0873 - accuracy: 0.9672 - 122ms/epoch - 4ms/step\n",
      "Epoch 174/200\n",
      "33/33 - 0s - loss: 0.0871 - accuracy: 0.9673 - 134ms/epoch - 4ms/step\n",
      "Epoch 175/200\n",
      "33/33 - 0s - loss: 0.0872 - accuracy: 0.9672 - 124ms/epoch - 4ms/step\n",
      "Epoch 176/200\n",
      "33/33 - 0s - loss: 0.0875 - accuracy: 0.9670 - 122ms/epoch - 4ms/step\n",
      "Epoch 177/200\n",
      "33/33 - 0s - loss: 0.0870 - accuracy: 0.9669 - 122ms/epoch - 4ms/step\n",
      "Epoch 178/200\n",
      "33/33 - 0s - loss: 0.0874 - accuracy: 0.9671 - 128ms/epoch - 4ms/step\n",
      "Epoch 179/200\n",
      "33/33 - 0s - loss: 0.0868 - accuracy: 0.9672 - 126ms/epoch - 4ms/step\n",
      "Epoch 180/200\n",
      "33/33 - 0s - loss: 0.0867 - accuracy: 0.9674 - 123ms/epoch - 4ms/step\n",
      "Epoch 181/200\n",
      "33/33 - 0s - loss: 0.0869 - accuracy: 0.9670 - 123ms/epoch - 4ms/step\n",
      "Epoch 182/200\n",
      "33/33 - 0s - loss: 0.0870 - accuracy: 0.9673 - 122ms/epoch - 4ms/step\n",
      "Epoch 183/200\n",
      "33/33 - 0s - loss: 0.0871 - accuracy: 0.9672 - 123ms/epoch - 4ms/step\n",
      "Epoch 184/200\n",
      "33/33 - 0s - loss: 0.0878 - accuracy: 0.9663 - 129ms/epoch - 4ms/step\n",
      "Epoch 185/200\n",
      "33/33 - 0s - loss: 0.0873 - accuracy: 0.9673 - 122ms/epoch - 4ms/step\n",
      "Epoch 186/200\n",
      "33/33 - 0s - loss: 0.0865 - accuracy: 0.9670 - 122ms/epoch - 4ms/step\n",
      "Epoch 187/200\n",
      "33/33 - 0s - loss: 0.0866 - accuracy: 0.9675 - 124ms/epoch - 4ms/step\n",
      "Epoch 188/200\n",
      "33/33 - 0s - loss: 0.0863 - accuracy: 0.9676 - 123ms/epoch - 4ms/step\n",
      "Epoch 189/200\n",
      "33/33 - 0s - loss: 0.0860 - accuracy: 0.9678 - 123ms/epoch - 4ms/step\n",
      "Epoch 190/200\n",
      "33/33 - 0s - loss: 0.0864 - accuracy: 0.9672 - 121ms/epoch - 4ms/step\n",
      "Epoch 191/200\n",
      "33/33 - 0s - loss: 0.0868 - accuracy: 0.9668 - 125ms/epoch - 4ms/step\n",
      "Epoch 192/200\n",
      "33/33 - 0s - loss: 0.0863 - accuracy: 0.9672 - 123ms/epoch - 4ms/step\n",
      "Epoch 193/200\n",
      "33/33 - 0s - loss: 0.0862 - accuracy: 0.9674 - 122ms/epoch - 4ms/step\n",
      "Epoch 194/200\n",
      "33/33 - 0s - loss: 0.0859 - accuracy: 0.9679 - 122ms/epoch - 4ms/step\n",
      "Epoch 195/200\n",
      "33/33 - 0s - loss: 0.0857 - accuracy: 0.9677 - 126ms/epoch - 4ms/step\n",
      "Epoch 196/200\n",
      "33/33 - 0s - loss: 0.0856 - accuracy: 0.9680 - 128ms/epoch - 4ms/step\n",
      "Epoch 197/200\n",
      "33/33 - 0s - loss: 0.0858 - accuracy: 0.9677 - 123ms/epoch - 4ms/step\n",
      "Epoch 198/200\n",
      "33/33 - 0s - loss: 0.0856 - accuracy: 0.9678 - 124ms/epoch - 4ms/step\n",
      "Epoch 199/200\n",
      "33/33 - 0s - loss: 0.0858 - accuracy: 0.9675 - 122ms/epoch - 4ms/step\n",
      "Epoch 200/200\n",
      "33/33 - 0s - loss: 0.0852 - accuracy: 0.9683 - 123ms/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(20, return_sequences=True,input_shape=(1,56)))\n",
    "    model.add(LSTM(20, return_sequences=True))\n",
    "    model.add(Dense(10, activation='softmax')) #for multiclass classification\n",
    "    #Compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',\n",
    "                  # metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "                  metrics=['accuracy']\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "#The LSTM input layer must be 3D.\n",
    "#The meaning of the 3 input dimensions are: samples, time steps, and features.\n",
    "#reshape input data\n",
    "X_train_array = array(X_train) #array has been declared in the previous cell\n",
    "print(len(X_train_array))\n",
    "X_train_reshaped = X_train_array.reshape(X_train_array.shape[0],1,56)\n",
    "\n",
    "#reshape output data\n",
    "X_test_array=  array(X_test)\n",
    "X_test_reshaped = X_test_array.reshape(X_test_array.shape[0],1,56) \n",
    "\n",
    "\n",
    "#institate the model\n",
    "model = build_model()\n",
    "\n",
    "\n",
    "#fit the model\n",
    "start = time.time()\n",
    "model.fit(X_train_reshaped, y_train, epochs=200, batch_size=2000,verbose=2)\n",
    "end_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 1s 946us/step - loss: 0.0898 - accuracy: 0.9644\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the neural network\n",
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "# loss, accuracy, f1s, precision, recall = model.evaluate(X_test_reshaped, y_test)\n",
    "end_predict = time.time()\n",
    "model_performance.loc['LSTM (Keras)'] = [accuracy, accuracy, accuracy, accuracy,end_train-start,end_predict-end_train,end_predict-start]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c4808_row0_col0, #T_c4808_row0_col1, #T_c4808_row0_col3 {\n",
       "  background-color: #d1493f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4808_row0_col2 {\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4808_row0_col4, #T_c4808_row0_col5, #T_c4808_row0_col6, #T_c4808_row1_col5, #T_c4808_row2_col0, #T_c4808_row2_col1, #T_c4808_row2_col2, #T_c4808_row2_col3, #T_c4808_row2_col5, #T_c4808_row3_col5 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4808_row1_col0, #T_c4808_row1_col1, #T_c4808_row1_col2, #T_c4808_row1_col3, #T_c4808_row2_col4, #T_c4808_row5_col5, #T_c4808_row5_col6 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4808_row1_col4 {\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4808_row1_col6 {\n",
       "  background-color: #3c4ec2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4808_row2_col6 {\n",
       "  background-color: #a7c5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4808_row3_col0, #T_c4808_row3_col1, #T_c4808_row3_col3 {\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4808_row3_col2 {\n",
       "  background-color: #9fbfff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4808_row3_col4 {\n",
       "  background-color: #89acfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4808_row3_col6 {\n",
       "  background-color: #536edd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4808_row4_col0, #T_c4808_row4_col1, #T_c4808_row4_col3 {\n",
       "  background-color: #81a4fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4808_row4_col2 {\n",
       "  background-color: #80a3fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4808_row4_col4 {\n",
       "  background-color: #9abbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4808_row4_col5 {\n",
       "  background-color: #97b8ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4808_row4_col6, #T_c4808_row6_col2 {\n",
       "  background-color: #a3c2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4808_row5_col0, #T_c4808_row5_col1, #T_c4808_row5_col3 {\n",
       "  background-color: #c5d6f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4808_row5_col2 {\n",
       "  background-color: #c4d5f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4808_row5_col4 {\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4808_row6_col0, #T_c4808_row6_col1, #T_c4808_row6_col3 {\n",
       "  background-color: #a5c3fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4808_row6_col4 {\n",
       "  background-color: #f6a586;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4808_row6_col5 {\n",
       "  background-color: #4f69d9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4808_row6_col6 {\n",
       "  background-color: #9bbcff;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c4808\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c4808_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_c4808_level0_col1\" class=\"col_heading level0 col1\" >Recall</th>\n",
       "      <th id=\"T_c4808_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_c4808_level0_col3\" class=\"col_heading level0 col3\" >F1-Score</th>\n",
       "      <th id=\"T_c4808_level0_col4\" class=\"col_heading level0 col4\" >time to train</th>\n",
       "      <th id=\"T_c4808_level0_col5\" class=\"col_heading level0 col5\" >time to predict</th>\n",
       "      <th id=\"T_c4808_level0_col6\" class=\"col_heading level0 col6\" >total time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c4808_level0_row0\" class=\"row_heading level0 row0\" >Extra Trees</th>\n",
       "      <td id=\"T_c4808_row0_col0\" class=\"data row0 col0\" >97.53%</td>\n",
       "      <td id=\"T_c4808_row0_col1\" class=\"data row0 col1\" >97.53%</td>\n",
       "      <td id=\"T_c4808_row0_col2\" class=\"data row0 col2\" >97.55%</td>\n",
       "      <td id=\"T_c4808_row0_col3\" class=\"data row0 col3\" >97.53%</td>\n",
       "      <td id=\"T_c4808_row0_col4\" class=\"data row0 col4\" >0.6</td>\n",
       "      <td id=\"T_c4808_row0_col5\" class=\"data row0 col5\" >0.1</td>\n",
       "      <td id=\"T_c4808_row0_col6\" class=\"data row0 col6\" >0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4808_level0_row1\" class=\"row_heading level0 row1\" >Random Forest</th>\n",
       "      <td id=\"T_c4808_row1_col0\" class=\"data row1 col0\" >97.68%</td>\n",
       "      <td id=\"T_c4808_row1_col1\" class=\"data row1 col1\" >97.68%</td>\n",
       "      <td id=\"T_c4808_row1_col2\" class=\"data row1 col2\" >97.69%</td>\n",
       "      <td id=\"T_c4808_row1_col3\" class=\"data row1 col3\" >97.68%</td>\n",
       "      <td id=\"T_c4808_row1_col4\" class=\"data row1 col4\" >1.5</td>\n",
       "      <td id=\"T_c4808_row1_col5\" class=\"data row1 col5\" >0.0</td>\n",
       "      <td id=\"T_c4808_row1_col6\" class=\"data row1 col6\" >1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4808_level0_row2\" class=\"row_heading level0 row2\" >Gradient Boosting Classifier</th>\n",
       "      <td id=\"T_c4808_row2_col0\" class=\"data row2 col0\" >95.85%</td>\n",
       "      <td id=\"T_c4808_row2_col1\" class=\"data row2 col1\" >95.85%</td>\n",
       "      <td id=\"T_c4808_row2_col2\" class=\"data row2 col2\" >95.86%</td>\n",
       "      <td id=\"T_c4808_row2_col3\" class=\"data row2 col3\" >95.85%</td>\n",
       "      <td id=\"T_c4808_row2_col4\" class=\"data row2 col4\" >41.2</td>\n",
       "      <td id=\"T_c4808_row2_col5\" class=\"data row2 col5\" >0.0</td>\n",
       "      <td id=\"T_c4808_row2_col6\" class=\"data row2 col6\" >41.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4808_level0_row3\" class=\"row_heading level0 row3\" >MLP</th>\n",
       "      <td id=\"T_c4808_row3_col0\" class=\"data row3 col0\" >96.39%</td>\n",
       "      <td id=\"T_c4808_row3_col1\" class=\"data row3 col1\" >96.39%</td>\n",
       "      <td id=\"T_c4808_row3_col2\" class=\"data row3 col2\" >96.41%</td>\n",
       "      <td id=\"T_c4808_row3_col3\" class=\"data row3 col3\" >96.40%</td>\n",
       "      <td id=\"T_c4808_row3_col4\" class=\"data row3 col4\" >10.4</td>\n",
       "      <td id=\"T_c4808_row3_col5\" class=\"data row3 col5\" >0.0</td>\n",
       "      <td id=\"T_c4808_row3_col6\" class=\"data row3 col6\" >10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4808_level0_row4\" class=\"row_heading level0 row4\" >MLP (Keras)</th>\n",
       "      <td id=\"T_c4808_row4_col0\" class=\"data row4 col0\" >96.25%</td>\n",
       "      <td id=\"T_c4808_row4_col1\" class=\"data row4 col1\" >96.25%</td>\n",
       "      <td id=\"T_c4808_row4_col2\" class=\"data row4 col2\" >96.25%</td>\n",
       "      <td id=\"T_c4808_row4_col3\" class=\"data row4 col3\" >96.25%</td>\n",
       "      <td id=\"T_c4808_row4_col4\" class=\"data row4 col4\" >12.3</td>\n",
       "      <td id=\"T_c4808_row4_col5\" class=\"data row4 col5\" >27.3</td>\n",
       "      <td id=\"T_c4808_row4_col6\" class=\"data row4 col6\" >39.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4808_level0_row5\" class=\"row_heading level0 row5\" >GRU (Keras)</th>\n",
       "      <td id=\"T_c4808_row5_col0\" class=\"data row5 col0\" >96.61%</td>\n",
       "      <td id=\"T_c4808_row5_col1\" class=\"data row5 col1\" >96.61%</td>\n",
       "      <td id=\"T_c4808_row5_col2\" class=\"data row5 col2\" >96.61%</td>\n",
       "      <td id=\"T_c4808_row5_col3\" class=\"data row5 col3\" >96.61%</td>\n",
       "      <td id=\"T_c4808_row5_col4\" class=\"data row5 col4\" >26.1</td>\n",
       "      <td id=\"T_c4808_row5_col5\" class=\"data row5 col5\" >98.2</td>\n",
       "      <td id=\"T_c4808_row5_col6\" class=\"data row5 col6\" >124.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4808_level0_row6\" class=\"row_heading level0 row6\" >LSTM (Keras)</th>\n",
       "      <td id=\"T_c4808_row6_col0\" class=\"data row6 col0\" >96.44%</td>\n",
       "      <td id=\"T_c4808_row6_col1\" class=\"data row6 col1\" >96.44%</td>\n",
       "      <td id=\"T_c4808_row6_col2\" class=\"data row6 col2\" >96.44%</td>\n",
       "      <td id=\"T_c4808_row6_col3\" class=\"data row6 col3\" >96.44%</td>\n",
       "      <td id=\"T_c4808_row6_col4\" class=\"data row6 col4\" >29.8</td>\n",
       "      <td id=\"T_c4808_row6_col5\" class=\"data row6 col5\" >6.8</td>\n",
       "      <td id=\"T_c4808_row6_col6\" class=\"data row6 col6\" >36.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x226a82e8400>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance.fillna(.90,inplace=True)\n",
    "model_performance.style.background_gradient(cmap='coolwarm').format({'Accuracy': '{:.2%}',\n",
    "                                                                     'Precision': '{:.2%}',\n",
    "                                                                     'Recall': '{:.2%}',\n",
    "                                                                     'F1-Score': '{:.2%}',\n",
    "                                                                     'time to train':'{:.1f}',\n",
    "                                                                     'time to predict':'{:.1f}',\n",
    "                                                                     'total time':'{:.1f}',\n",
    "                                                                     })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
